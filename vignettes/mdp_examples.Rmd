---
title: "The MDP package in R (some examples)"
author: "Lars Relund <lars@relund.dk>"
date: '`r Sys.Date()`'
output:
  html_document: default
  pdf_document:
    keep_tex: no
bibliography: mdp_examples_files/litt.bib
vignette: |
  %\VignetteIndexEntry{MDP examples} 
  %\VignetteEngine{knitr::rmarkdown} 
  \usepackage[utf8]{inputenc}
---

# Introduction

The MDP package in R is a package for solving Markov decision processes (MDPs)
with discrete time-steps, states and actions. Both ordinary (@Puterman94}
and hierarchial MDPs (@Kristensen00) can be solved. In this paper we use
the term MDP for both types of MDPs.

Generating and solving an MDP is done in two steps. First, the MDP is generated
and saved in a set of binary files. Next, you load the MDP into memory from the
binary files and solve it.

The package uses algorithms based on the *state-expanded directed
hypergraph* of the MDP (@Relund06) which are all implemented in `C++` for
fast running times. Under development is also support for MLHMP which is a Java
implementation of algorithms for solving MDPs (@Kristensen03). A
hypergraph representing an MDP with time-horizon $N=5$ is shown in
Figure 1. Each node corresponds to a specific state in the
MDP and a directed hyperarc is defined for each possible action. For instance,
node $v_{2,1}$ corresponds to a state number 1 at stage 2. The two hyperarcs
with head in node $v_{3,0}$ show that two actions are possible given state
number 0 at stage 3. Action `mt` corresponds to a deterministic
transition to state number zero at stage 4 and action `nmt` corresponds
to a transition to state number 0 or 1 at stage 4 with a certain probability
greater than zero.

![Figure 1: A state-expanded hypergraph for an MDP.](mdp_examples_files/state_hgf.png)
*Figure 1: A state-expanded hypergraph for an MDP with time horizon $N=5$. At stage $n$ each node $v_{n,i}$ corresponds to a state in $\mathcal{S}_n$. The hyperarcs correspond to actions, e.g. if the system at stage $3$ is in state number 1 then there are two possible actions. Action `mt} results in a deterministic transition to state zero (because there is only one tail) at stage $4$ and `nmt` results in a transition to either state number 1 or 2 with a certain probability.*

States and actions can be identified using either an *unique id* or *state vector*
$\nu$. In an ordinary MDP the index vector consists of the stage and state
number, i.e. state corresponding to node $v_{3,1}$ in Figure 1 is
uniquely identified using $\nu=(n,s)=(3,1)$. Similar, action `buy` is
uniquely identified using $\nu=(n,s,a)=(0,0,0)$. Note that numbers in an **index always
start from zero**.

A hierarchical MDP is an MDP with parameters defined in a special way, but
nevertheless in accordance with all usual rules and conditions relating to such
processes (@Kristensen00). The basic idea of the hierarchical structure is
that stages of the process can be expanded to a so-called *child process*,
which again may expand stages further to new child processes leading to
multiple levels. To illustrate consider the MDP shown in Figure 2. The
process has three levels. At `Level 2` we have a set of ordinary MDPs
with finite time-horizon (one for each oval box) which all can be represented
using a state-expanded hypergraph (hyperarcs not shown, only hyperarcs
connecting processes are shown). An MDP at `Level 2` is uniquely defined
by a given state $s$ and action $a$ of its *parent process* at
`Level 1` (illustrated by the arcs with head and tail node at
`Level 1` and `Level 2`, respectively). Moreover, when a child
process at `Level 2` terminates a transition from a state $s\in
\mathcal{S}_{N}$ of the child process to a state at the next stage of the
parent process occur (illustrated by the (hyper)arcs having head and tail at
`Level 2` and `Level 1`, respectively). Since a child process is
always defined by a stage, state and action of the parent process we have that
for instance a state at level 1 have an index vector
$\nu=(n_{0},s_{0},a_{0},n_{1},s_{1})$.

![Figure 2: A state-expanded hypergraph for an HMDP.](mdp_examples_files/hmdp_index.png)
*Figure 2: A hypergraph representation of the first stage of a hierarchical MDP. Level 0 indicate the
founder level, and the nodes indicates states at the different levels. A child
process (oval box) is represented using its state-expanded hypergraph (hyperarcs
not shown) and is uniquely defined by a given state and action of its parent
process.*

In general a state $s$ and action $a$ at level $l$ can be uniquely identified
using
$$
\begin{aligned}
\nu_{s}&=(n_{0},s_{0},a_{0},n_{1},s_{1},\ldots,n_{l},s_{l}) \\
\nu_{a}&=(n_{0},s_{0},a_{0},n_{1},s_{1},\ldots,n_{l},s_{l},a_{l}).
\end{aligned}
$$
The index vector of three states is illustrated in Figure 2.

Another way to identify a state or action is using an id number. Id numbers can
be seen when printing information about the model i R. This will be further
clarified in the example in the next section.

Now let us have a look at package. The newest stable version of the package can be installed from R-Forge
using
```{r Install r-forge, echo=TRUE, eval=FALSE}
install.packages("MDP2",repos="http://r-forge.r-project.org")
```
or the newest development version from GitHub 
```{r Install github, echo=TRUE, eval=FALSE}
install_github("relund/mdp")
```
We load the package using 
```{r Load package,echo=TRUE}
library(MDP2)
```
Help about the package can be seen by writing

```{r Package help, echo=TRUE, eval=FALSE}
?MDP2
```
We illustrate the package capabilities by some examples in the next sections.



# Ordinary MDP with finite time-horizon

  $\left(n,s\right)$  |  $\left(1,0\right)$  |  $\left(1,1\right)$  |  $\left(2,0\right)$  |  $\left(2,1\right)$  |  $\left(3,0\right)$  |  $\left(3,1\right)$
  ------------------  |  ------------------  |  ------------------  |  ------------------  |  ------------------  |  ------------------  |  ------------------
  $reward$            | 70 | 50 | 70 | 50 | 70 | 50
  $s^{\prime}$        | $\left\{0,1\right\}$ | $\left\{1,2\right\}$ | $\left\{0,1\right\}$ | $\left\{1,2\right\}$ | $\left\{0,1\right\}$ | $\left\{1,2\right\}$
  $p_{n}\left(\cdot\mid s,nmt\right)$ | $\left\{\frac{6}{10},\frac{4}{10}\right\}$ | $\left\{\frac{6}{10},\frac{4}{10}\right\}$ | $\left\{\frac{5}{10},\frac{5}{10}\right\}$ | $\left\{\frac{5}{10},\frac{5}{10}\right\}$ | $\left\{\frac{2}{10},\frac{8}{10}\right\}$ | $\left\{\frac{2}{10},\frac{8}{10}\right\}$

Table: Input data for the machine replacement problem given action `nmt`.

Consider the machine replacement example from  @Relund06 where the
machine is always replaced after 4 years. The state of the machine may be:
good, average, and not working. Given the machine's state we may maintain the
machine. In this case the machine's state will be good at the next decision
epoch. Otherwise, the machine's state will not be better at next decision
epoch. When the machine is bought it may be either in state good or average.
Moreover, if the machine is not working it must be replaced.

The problem of when to replace the machine can be modelled using a Markov
decision process with $N=5$ decision epochs. We use system states `good`
(state 0),  `average` (state 1), `not working` (state 2) and
dummy state `replaced` together with actions buy (`buy`),
maintain (`mt`), no maintenance (`nmt`), and replace
(`rep`).

The set of states at stage zero $S_{0}$ contains a single dummy state $s_{0}$
representing the machine before knowing its initial state. The only possible
action is `buy`.

The cost of buying the machine is 100 with transition probability of 0.7 to
state `good` and 0.3 to state `average`. The reward (scrap value)
of replacing a machine is 30, 10, and 5 in state 0, 1, and 2, respectively. The
reward of the machine given action `mt` becomes 55, 40, and 30 given
state 0, 1, and 2, respectively. Moreover, the system enters state 0 with
probability 1 at the next stage. Finally, Table 1 shows the reward,
transition states and probabilities given action `nmt`.

The state-expanded hypergraph is shown in Figure 1. It contains a
hyperarc for each possible action $a$ given stage $n$ and state $s\in S_{n}$.
The head node of a hyperarc corresponds to the state of the system before
action $a$ is taken and the tail nodes to the possible system states after
action $a$ is taken.


## Generating the MDP

We generate the model in R using the `binaryMDPWriter`:

```{r Generate replacement model,echo=TRUE}
prefix<-"machine_"
w <- binaryMDPWriter(prefix)
w$setWeights(c("Net reward"))
w$process()
	w$stage()   # stage n=0
		w$state(label="Dummy")          # v=(0,0)
			w$action(label="buy", weights=-100, prob=c(1,0,0.7, 1,1,0.3), end=T)
		w$endState()
	w$endStage()
	w$stage()   # stage n=1
		w$state(label="good")           # v=(1,0)
			w$action(label="mt", weights=55, prob=c(1,0,1), end=T)
			w$action(label="nmt", weights=70, prob=c(1,0,0.6, 1,1,0.4), end=T)
		w$endState()
		w$state(label="average")        # v=(1,1)
			w$action(label="mt", weights=40, prob=c(1,0,1), end=T)
			w$action(label="nmt", weights=50, prob=c(1,1,0.6, 1,2,0.4), end=T)
		w$endState()
	w$endStage()
	w$stage()   # stage n=2
		w$state(label="good")           # v=(2,0)
			w$action(label="mt", weights=55, prob=c(1,0,1), end=T)
			w$action(label="nmt", weights=70, prob=c(1,0,0.5, 1,1,0.5), end=T)
		w$endState()
		w$state(label="average")        # v=(2,1)
			w$action(label="mt", weights=40, prob=c(1,0,1), end=T)
			w$action(label="nmt", weights=50, prob=c(1,1,0.5, 1,2,0.5), end=T)
		w$endState()
		w$state(label="not working")    # v=(2,2)
			w$action(label="mt", weights=30, prob=c(1,0,1), end=T)
			w$action(label="rep", weights=5, prob=c(1,3,1), end=T)
		w$endState()
	w$endStage()
	w$stage()   # stage n=3
		w$state(label="good")           # v=(3,0)
			w$action(label="mt", weights=55, prob=c(1,0,1), end=T)
			w$action(label="nmt", weights=70, prob=c(1,0,0.2, 1,1,0.8), end=T)
		w$endState()
		w$state(label="average")        # v=(3,1)
			w$action(label="mt", weights=40, prob=c(1,0,1), end=T)
			w$action(label="nmt", weights=50, prob=c(1,1,0.2, 1,2,0.8), end=T)
		w$endState()
		w$state(label="not working")    # v=(3,2)
			w$action(label="mt", weights=30, prob=c(1,0,1), end=T)
			w$action(label="rep", weights=5, prob=c(1,3,1), end=T)
		w$endState()
		w$state(label="replaced")       # v=(3,3)
			w$action(label="Dummy", weights=0, prob=c(1,3,1), end=T)
		w$endState()
	w$endStage()
	w$stage()   # stage n=4
		w$state(label="good", end=T)        # v=(4,0)
		w$state(label="average", end=T)     # v=(4,1)
		w$state(label="not working", end=T) # v=(4,2)
		w$state(label="replaced", end=T)    # v=(4,3)
	w$endStage()
w$endProcess()
w$closeWriter()
```

A set of binary files (all with prefix `machine\_`) containing the model
have now been generated. Note how the model is generated in a hierarchical way.
A process contain stages which contain states which again contain actions. An
action is defined by a set of weights (in this case the net reward) and a set
of transition probabilities. The probabilities are defined using a vector of
the form $(q_{0},i_{0},p_{0},\ldots,q_{r},i_{r},p_{r})$ stating that $r$
transitions are possible. Each triple $(q_{j},i_{j},p_{j})$ define a
transition. The number $q_{j}\in \{0,1,2\}$ is the scope of the transition. If
$q_{j} = 0$ then we make a transition to the next stage in the parent process,
if $q_{j} = 1$ we make a transition to the next stage in the current process
and if $q_{j} = 2$ we make a transition to the first stage in the child
process. The number $i_{j}$ define which state index we consider at the next
stage, e.g. if $i_{j}=0$ we consider the state with index 0 (remember index
start from zero). Finally, $p_{j}$ is the probability. For instance,
$(q_{j},i_{j},p_{j})=(1,3,0.2)$ specify that we have a transition with
probability 0.2 to the state with index 3 at the next stage of the current
process.


## Getting an overview

Various information about the whole model can be seen in R:

```{r Load model (machine rep),echo=TRUE}
mdp<-loadMDP(prefix)    # load the model
mdp                     # general info
```
The `mdp` object is a list containing basic information about the model and a pointer
to the `C++` object containing the model.

```{r Info (machine rep),echo=TRUE}
info<-infoMDP(mdp)
info$stateDF
info$actionDF
```
The list `info` contain an element for each state with subelements for each action. An summary of 
the states and actions can be seen using the elements `stateDF` and `actionDF`
Note that the data frame for the states show both each states unique id (a
single number) and state vector (`stateStr`column).
For each action is assigned and action index which is the number to be appended to the state vector 
to get the action vector.

## Finding the optimal policy

A finite-horizon MDP can be solved using value iteration. 
 Next, we solve the MDP using value
iteration:
%
```{r Optimize MDP (machine rep),echo=TRUE}
wLbl<-"Net reward"             # the weight we want to optimize
scrapValues<-c(30,10,5,0)     # scrap values (the values of the 4 states at stage 4)
valueIte(mdp, wLbl, termValues=scrapValues)
```

The MDP has now been optimized. The optimal policy can be extracted using:

```{r Get policy,echo=TRUE}
getPolicy(mdp)
```

Note that the states at the last stage have no optimal actions.  


## Evaluating a specific policy

We may evaluate a certain policy, e.g. the policy always to maintain the machine:

```{r Set policy (machine rep),echo=TRUE,eval=TRUE}
policy<-data.frame(sId=c(8,11),aIdx=c(0,0))
setPolicy(mdp, policy)
```

If the policy matrix does not contain all states then the actions from the
previous optimal policy are used. Now let us calculate the expected reward of
that policy:

```{r Calc reward (machine rep),echo=TRUE}
calcWeights(mdp, wLbl, termValues=scrapValues)
getPolicy(mdp)    
```


# Ordinary MDP with infinite time-horizon

Consider an example from livestock farming. For a sow it is relevant to consider at regular time 
intervals whether to keep the sow for a period more or replace it by a new sow. Let a stage denote the
time between two litters. At the time of a stage we observe the state of the
sow which in this simple example is the current litter size `small`, `average` or `big`.

Two actions are possible, namely, `keep` or `replace`. Given an
action 3 weights are defined the duration, net reward and the number of
piglets. The weights and transition probabilities of an action are specified
explicit when we generate the MDP:

```{r Generate sow MDP, echo=TRUE}
prefix="sow_"
w<-binaryMDPWriter(prefix)
w$setWeights(c("Duration", "Net reward", "Piglets"))
w$process()
	w$stage()
		w$state(label="Small litter")
			w$action(label="Keep",weights=c(1,10000,8),prob=c(1,0,0.6, 1,1,0.3, 1,2,0.1))
			w$endAction()
			w$action(label="Replace",weights=c(1,9000,8),prob=c(1,0,1/3, 1,1,1/3, 1,2,1/3))
			w$endAction()
		w$endState()
		w$state(label="Average litter")
			w$action(label="Keep",weights=c(1,12000,11),prob=c(1,0,0.2, 1,1,0.6, 1,2,0.2))
			w$endAction()
			w$action(label="Replace",weights=c(1,11000,11),prob=c(1,0,1/3, 1,1,1/3, 1,2,1/3))
			w$endAction()
		w$endState()
		w$state(label="Big litter")
			w$action(label="Keep",weights=c(1,14000,14),prob=c(1,0,0.1, 1,1,0.3, 1,2,0.6))
			w$endAction()
			w$action(label="Replace",weights=c(1,13000,14),prob=c(1,0,1/3, 1,1,1/3, 1,2,1/3))
			w$endAction()
		w$endState()
	w$endStage()
w$endProcess()
w$closeWriter()
```

Note that since we only have one stage at the founder level (level 0) the MDP
have an infinite time-horizon. That is, the MDP model a sow and all it
successors (when a sow is replaced, a new is always inserted).

Let us have a overview over the model

```{r Model info (sow rep), echo=TRUE}
mdp<-loadMDP(prefix)    # load the model
info<-infoMDP(mdp)
info$stateDF
info$actionDF
```


## Finding the optimal policy under different criteria

Let us try to optimize our model under the expected discounted reward
criterion. Here two optimization techniques are possible. Let us first have a
look at value iteration which provide an approximate solution.

```{r Value ite (sow rep), echo=TRUE}
## solve the MDP using value iteration
wLbl<-"Net reward"         # the weight we want to optimize
durLbl<-"Duration"         # the duration/time label
rate<-0.1               # discount rate
rateBase<-1             # rate base
valueIte(mdp, wLbl, durLbl, rate, rateBase, maxIte = 10000, eps = 0.00001)
getPolicy(mdp)     # optimal policy for each sId
```

First note that the we optimize the MDP for a specific interest rate which
according to a rate basis, i.e. if the rate is 0.1 and the rate base is 4 then
the discount rate over one time unit is $\exp(-0.1/4) =
`r round(exp(-0.1/4),4)`$. The discount rate over t time units then becomes
$$
	\delta(t) = \exp(-rate/rateBase)^{t}.
$$
Second, the parameter `maxIte` denote an upper bound on the number of iterations.
Finally, the parameter `eps` denote the $\epsilon$ for stopping the algorithm. If
the maximum difference between the expected discounted reward of 2 states is
below $\epsilon$ then the algorithm stops, i.e the policy becomes epsilon
optimal (see @Puterman94 p161).

Let us have a look at how value iteration performs for each iteration.

```{r Value ite steps (sow rep), echo=TRUE, results='hide'}
termValues<-c(0,0,0)
iterations<-1:211
df<-data.frame(n=iterations,a1=NA,V1=NA,D1=NA,a2=NA,V2=NA,D2=NA,a3=NA,V3=NA,D3=NA)
for (i in iterations) {
	valueIte(mdp, wLbl, durLbl, rate, rateBase, maxIte = 1, eps = 0.00001, termValues)
	a<-getPolicy(mdp)
	res<-rep(NA,10)
	res[1]<-i
	res[2]<-a$actionLabel[1]
	res[3]<-round(a$weight[1],2)
	res[4]<-round(a$weight[1]-termValues[1],2)
	res[5]<-a$actionLabel[2]
	res[6]<-round(a$weight[2],2)
	res[7]<-round(a$weight[2]-termValues[2],2)
	res[8]<-a$actionLabel[3]
	res[9]<-round(a$weight[3],2)
	res[10]<-round(a$weight[3]-termValues[3],2)
	df[i,]<-res
	termValues<-a$weight
}
```

```{r}
df[c(1:3,51:53,151:153,210:211),]
```


Note value iteration converges very slowly to the optimal value.

Another optimization technique is policy iteration which finds an optimal
policy. Let us solve the MDP under the expected discount criterion.

```{r Policy ite discount (sow rep), echo=TRUE}
policyIteDiscount(mdp, wLbl, durLbl, rate, rateBase)
policy<-getPolicy(mdp)
rpo<-calcRPO(mdp, wLbl, iA=c(0,0,0), criterion="discount", dur=durLbl, rate=rate, rateBase=rateBase)
policy<-merge(policy,rpo)
policy
```

First, note that policy iteration converges fast only 3 iterations are needed.
Second, we also here try to calculate the *retention payoff* (*RPO*)
or opportunity cost with respect to action `keep} (action index 0). The
RPO is the discounted gain of keeping the sow until her optimal replacement
time instead of replacing her now. For instance if we consider a sow with a big
litter we loose `r round(policy$rpo[3],0)` by replacing the sow instead
keeping her to her until her optimal replacement time. That is, if the RPO is
positive the optimal decision is to keep the sow and if the RPO is negative the
optimal decision is to replace the sow.

Other criteria can also be optimized using policy iteration. For instance we
can maximize the average reward over time:

```{r Policy ite ave reward over time (sow rep),echo=true}
g<-policyIteAve(mdp, wLbl, durLbl)
policy<-getPolicy(mdp)
rpo<-calcRPO(mdp, wLbl, iA=c(0,0,0), criterion="average", dur=durLbl, rate=rate, rateBase=rateBase)
policy<-merge(policy,rpo)
policy
```

Here $g$ is the average reward pr time unit and the weights are relative values
compared to the `big litter` state.

We may also maximize the average reward over piglets:

```{r Policy ite ave reward over litter(sow rep), echo=TRUE}
durLbl<-"Piglets"
g<-policyIteAve(mdp, wLbl, dur=durLbl)
policy<-getPolicy(mdp)
rpo<-calcRPO(mdp, wLbl, iA=c(0,0,0), criterion="average", dur=durLbl, rate=rate, rateBase=rateBase)
policy<-merge(policy,rpo)
policy
```

Here $g$ is the average reward pr piglet and the weights are relative values
compared to the `big litter` state.



## Calculating other key figures for the optimal policy

Consider the optimal policy under the expected discounted reward criterion:

```{r Policy ite discount (sow rep) again, echo=TRUE}
policyIteDiscount(mdp, wLbl, durLbl, rate, rateBase)
policy<-getPolicy(mdp)
rpo<-calcRPO(mdp, wLbl, iA=c(0,0,0), criterion="discount", dur=durLbl, rate=rate, rateBase=rateBase)
policy<-merge(policy,rpo)
policy
```

Since other weights are defined for each action we can calculate the average
number of piglets per time unit under the optimal policy:

```{r Piglets/time (sow rep), echo=TRUE}
calcWeights(mdp, w="Piglets", criterion="average", dur = "Duration")
```

or the average reward per piglet:

```{r Reward/piglet (sow rep), echo=TRUE}
calcWeights(mdp, w="Net reward", criterion="average", dur = "Piglets")
```




# Hierarchical MDP with infinite time-horizon

We consider problem from livestock farming, namely the cow replacement problem where we want to 
represent the age of the
cow, i.e. the lactation number of the cow. During a lactation a cow may have a
high, average or low yield. We assume that a cow is always replaced after 4
lactations.

In addition to lactation and milk yield we also want to take the genetic merit
into account which is either bad, average or good. When a cow is replaced we
assume that the probability of a bad, average or good heifer is equal.

We formulate the problem as a hierarchical MDP with 2 levels. At level 0 the
states are the genetic merit and the length of a stage is a life of a cow. At
level 1 a stage describe a lactation and states describe the yield. Decisions
at level 1 are `keep` or `replace`.

Note the MDP runs over an infinite time-horizon at the founder level where each
state (genetic merit) define an ordinary MDP at level 1 with 4 lactations.

## Generating the MDP

To generate the MDP we need to know the weights and transition probabilities which
are provided in a csv file. To ease the understanding we provide 2 functions
for reading from the csv:

```{r Generate cow MDP functions,echo=true}
cowDf<-read.csv("mdp_examples_files/cow.csv")
head(cowDf)

lev1W<-function(s0Idx,n1Idx,s1Idx,a1Lbl) {
	r<-subset(cowDf,s0==s0Idx & n1==n1Idx & s1==s1Idx & label==a1Lbl)
	return(as.numeric(r[5:7]))
}
lev1W(2,2,1,'Keep')     # good genetic merit, lactation 2, avg yield, keep action

lev1Pr<-function(s0Idx,n1Idx,s1Idx,a1Lbl) {
	r<-subset(cowDf,s0==s0Idx & n1==n1Idx & s1==s1Idx & label==a1Lbl)
	return(as.numeric(r[8:16]))
}
lev1Pr(2,2,1,'Replace') # good genetic merit, lactation 2, avg yield, replace action
```



```{r Generate cow MDP,echo=true}
lblS0<-c('Bad genetic level','Avg genetic level','Good genetic level')
lblS1<-c('Low yield','Avg yield','High yield')
prefix<-"cow_"
w<-binaryMDPWriter(prefix)
w$setWeights(c("Duration", "Net reward", "Yield"))
w$process()
	w$stage()   # stage 0 at founder level
		for (s0 in 0:2) {
			w$state(label=lblS0[s0+1])   # state at founder
				w$action(label="Keep", weights=c(0,0,0), prob=c(2,0,1))   # action at founder
					w$process()
						w$stage()   # dummy stage at level 1
							 w$state(label="Dummy")
								w$action(label="Dummy", weights=c(0,0,0), prob=c(1,0,1/3, 1,1,1/3, 1,2,1/3))
								w$endAction()
							 w$endState()
						w$endStage()
						for (d1 in 1:4) {
							w$stage()   # stage at level 1
								for (s1 in 0:2) {
									w$state(label=lblS1[s1+1])
										if (d1!=4) {
											w$action(label="Keep", weights=lev1W(s0,d1,s1,"Keep"), prob=lev1Pr(s0,d1,s1,"Keep"))
											w$endAction()
										}
										w$action(label="Replace", weights=lev1W(s0,d1,s1,"Replace"), prob=lev1Pr(s0,d1,s1,"Replace"))
										w$endAction()
									w$endState()
								}
							w$endStage()
						}
					w$endProcess()
				w$endAction()
			w$endState()
		}
	w$endStage()
w$endProcess()
w$closeWriter()
```


## Finding the optimal policy

We find the optimal policy under the expected discounted reward criterion the
MDP using policy iteration:

```{r Optimize (cow)}
## solve under discount criterion
mdp<-loadMDP(prefix)
wLbl<-"Net reward"         # the weight we want to optimize (net reward)
durLbl<-"Duration"         # the duration/time label
rate<-0.1               # discount rate
rateBase<-1             # rate base, i.e. given a duration of t the rate is
policyIteDiscount(mdp, wLbl, durLbl, rate, rateBase)
policy<-getPolicy(mdp, stateStr = TRUE)
rpo<-calcRPO(mdp, wLbl, iA=rep(0,42), criterion="discount", dur=durLbl, rate=rate, rateBase=rateBase)
policy<-merge(policy,rpo)
policy
```







# References
