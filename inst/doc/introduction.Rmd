---
title: "An introduction to the MDP package in R"
author: "Lars Relund <lars@relund.dk>"
date: "`r Sys.Date()`"
bibliography: litt.bib
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

<style> 
p {text-align: justify;} 
//.sourceCode {background-color: white;}
pre {
  border-style: solid;
  border-width: 1px;
  border-color: grey;
  //background-color: grey !important;
}
</style>

<!-- scale math down -->
<script type="text/x-mathjax-config"> 
    MathJax.Hub.Config({ 
        "HTML-CSS": { scale: 80 }
        });
</script>

```{r setup, include=FALSE}
library(knitr)
options(formatR.arrow = TRUE, width=90) # , scipen=999, digits=5
#thm <- knit_theme$get("edit-kwrite")   # whitengrey, bright, print, edit-flashdevelop, edit-kwrite
#knit_theme$set(thm)
opts_chunk$set(fig.align='center', 
               fig.width=10, fig.height=5, 
               fig.show='hold', 
               out.extra='style="max-width:100%;"',
               tidy = TRUE,
               prompt=T,
               comment=NA,
               cache=F, 
               background="red")
```


The MDP package in R is a package for solving Markov decision processes (MDPs) with discrete 
time-steps, states and actions. Both traditional MDPs [@Puterman94], semi-Markov decision processes (semi-MDPs) [@Tijms03] and 
hierarchial-MDPs (HMDPs) [@Kristensen00] can be solved under a finite and infinite time-horizon. 
<!-- In this paper we use the abbreviation MDP for all types. -->

Building and solving an MDP is done in two steps. First, the MDP is built and saved in a set 
of binary files. Next, you load the MDP into memory from the binary files and apply various
algorithms to the model.

In the package are implemented well-known algorithms such as policy iteration and value iteration
under different critria e.g. average reward per time unit and expected total discounted reward. The
model is stored using an underlying data structure based on the *state-expanded directed hypergraph*
of the MDP (@Relund06) implemented in `C++` for fast running times. <!-- Under development is also
support for MLHMP which is a Java implementation of algorithms for solving MDPs (@Kristensen03). 
-->

The newest version of the package can be installed from GitHub 
```{r Install github, echo=TRUE, eval=FALSE}
install_github("relund/mdp")
```
We load the package using 
```{r Load package, echo=TRUE, warning=FALSE, results='hide', message=FALSE}
library(MDP2)
```
Help about the package can be seen by writing
```{r Package help, echo=TRUE, eval=FALSE}
?MDP2
```

To illustrate the package capabilities, we use two examples, namely, a semi-MDP and a HMDP. Before each example a short introduction to these two models are given.


## An infinite Semi-MDP 

An *infinite-horizon semi-MDP* considers a sequential decision problem over an infinite number of
*stages*. Let $I$ denote the finite set of system states at stage $n$. Note we assume that the
semi-MDP is *homogeneous*, i.e the state space is independent of stage number. When *state* $i \in
I$ is observed, an *action* $a$ from the finite set of allowable actions $A(i)$ must be chosen, and
this decision generates *reward* $r(i,a)$. Moreover, let $\tau(i,a)$ denote the *stage length* of
action $a$, i.e. the expected time until the next decision epoch (stage $n+1$) given action $a$ and
state $i$. Finally, let $p_{ij}(a)$ denote the *transition probability* of obtaining state $j\in I$ 
at stage $n+1$ given that action $a$ is chosen in state $i$ at stage $n$. A policy is a decision 
rule/function that assigns to each state in the process an action.

We consider example 6.1.1 in @Tijms03. At the beginning of each day a piece of equipment is
inspected to reveal its actual working condition. The equipment will be found in one of the working
conditions $i = 1,\ldots, N$ where the working condition $i$ is better than the working condition 
$i+1$. The equipment deteriorates in time. If the present working condition is $i$ and no repair is
done, then at the beginning of the next day the equipment has working condition $j$ with probability
$q_{ij}$. It is assumed that $q_{ij}=0$ for $j<i$ and $\sum_{j\geq i}q_{ij}=1$. The working
condition $i=N$ represents a malfunction that requires an enforced repair taking two days. For the
intermediate states $i$ with $1<i<N$ there is a choice between preventively repairing the equipment
and letting the equipment operate for the present day. A preventive repair takes only one day. A
repaired system has the working condition $i=1$. The cost of an enforced repair upon failure is
$C_{f}$ and the cost of a preemptive repair in working condition $i$ is $C_{p}(i)$. We wish to
determine a maintenance rule which minimizes the long-run average repair cost per day.

To formulate this problem as an infinite horizon semi-MDP the set of possible states of the system
is chosen as
$$
I=\{1,2,\ldots,N\}.
$$
State $i$ corresponds to the situation in which an inspection reveals working condition $i$. Define actions
$$
a=\left\{\begin{array}{ll}
0 & \text{if no repair.}\\
1 & \text{if preventive repair.}\\
2 & \text{if forced repair.}\\
\end{array}\right.
$$
The set of possible actions in state $i$ is chosen as $A(1)=\{0\},\ A(i)=\{0,1\}$ for $1<i<N, 
A(N)=\{2\}$. The one-step transition probabilities $p_{ij}(a)$ are given by $p_{ij}(0) = q_{ij}$ for
$1\leq i<N$, $p_{i1}(1) = 1$ for $1<i<N$, and zero otherwise. The one-step costs $c_{i}(a)$ are
given by $c_{i}(0)=0,\ c_{i}(1)=C_{p}(i)$ and $c_{N}(2)=C_{f}$. The stage length until next decision
epoch are $\tau_i(a) = 1, 0\leq i < N$ and $\tau_N(a) = 2$.

Assume that the number of possible working conditions equals $N=5$. The repair costs are given by $C_{f}=10,\ C_{p}(2)=7,\ C_{p}(3)=7$ and $C_{p}(4)=5$. The deterioration probabilities $q_{ij}$ are given by

```{r parameters,  include=FALSE}
N<-5; Cf<- -10; Cp<-c(0,-7,-7,-5) # use negative numbers since the MDP optimize based on rewards
Q <- matrix(c(
   0.90, 0.10, 0, 0, 0,
   0, 0.80, 0.10, 0.05, 0.05,
   0, 0, 0.70, 0.10, 0.20,
   0, 0, 0, 0.50, 0.50), nrow=4, byrow=T) 
```

```{r Qtable, results='asis', echo=FALSE}
rownames(Q)<-1:4
colnames(Q)<-1:5
knitr::kable(Q, row.names = T)
```


```{r states, include=FALSE}
states<-data.frame(id=1:N-1,label=paste0("i=",1:N), stringsAsFactors = F)
states
```

```{r transPr, include=FALSE}
# transform state to id
state2Id<-function(i) return(i-1)

# input state i and action a
transPr<-function(a,i) {
   if (a==0) {
      pr<-Q[i,]
      iN<-which(pr>0)
      pr<-pr[iN]       # only consider trans pr > 0
   }
   if (a>0) {
      pr<-1
      iN<-1
   }
   return(list(pr=pr,id=state2Id(iN)))
}
transPr(0,1)
```

```{r buildMDP1, include=FALSE}
# Build the model which is stored in a set of binary files
w<-binaryMDPWriter("hct611-1_")
w$setWeights(c("Duration","Net reward"))
w$process()
   w$stage()
      w$state(label="i=1")
         dat<-transPr(0,1)
         w$action(label="no repair", weights=c(1,0), pr=dat$pr, id=dat$id, end=T)
      w$endState()
      for (ii in 2:(N-1) ) {
         w$state(label=states$label[ii])
            dat<-transPr(0,ii)
            w$action(label="no repair", weights=c(1,0), pr=dat$pr, id=dat$id, end=T)
            dat<-transPr(1,ii)
            w$action(label="preventive repair", weights=c(1,Cp[ii]), pr=dat$pr, id=dat$id, end=T)
         w$endState()
      }
      w$state(label=paste0("i=",N))
         dat<-transPr(2,N)
         w$action(label="forced repair", weights=c(2,Cf), pr=dat$pr, id=dat$id, end=T)
      w$endState()
   w$endStage()
w$endProcess()
w$closeWriter()
```

A state-expanded hypergraph representing the MDP with infinite time-horizon is shown below. Each
node corresponds to a specific state in the MDP and is given an *unique id* (**id must always start
from zero**). A directed hyperarc is defined for each possible action. For instance, the state/node
with id 1 corresponding to working condition $i=2$ and the two hyperarcs with head in this node
corresponds to the two actions preventive and no repair. Note the tails of a hyperare represent a
possible transition ($p_{ij}(a)>0$).


```{r plotHgf, echo=FALSE, results='hide', message=FALSE}
mdp<-loadMDP("hct611-1_")
dat<-infoMDP(mdp,withHarc = TRUE)
stateDF<-dat$stateDF
stateDF$label<-paste0(c(1:N,1:N)-1,": i=",c(1:N,1:N) )
#stateDF$label[1:N]<-""
stateDF$gId<-c(seq(2,10,by=2),seq(1,9,by=2))
actionDF<-dat$harcDF
actionDF$label<-dat$actionDF$label
actionDF$lwd<-0.5
actionDF$lty[actionDF$label=="no repair"]<-1
actionDF$lty[actionDF$label=="preventive repair"]<-1
actionDF$lty[actionDF$label=="forced repair"]<-1
actionDF$col[actionDF$label=="no repair"]<-"darkorange1"
actionDF$col[actionDF$label=="preventive repair"]<-"deepskyblue3"
actionDF$col[actionDF$label=="forced repair"]<-"chartreuse4"
actionDF$highlight<-FALSE
par(mai=c(0,0,0,0))
plotHypergraph(gridDim=c(N,2), states = stateDF, actions = actionDF)
```

To build the semi-MDP in R, we use the `binaryMDPWriter` where the model can be built using either using matrices or an hierachical structure. We first illustrate how to use the hierachical structure. First, we load the parameters:

```{r view_param, eval=FALSE, ref.label='parameters', echo=TRUE}
```

and make a data frame for the states:

```{r view_states, eval=TRUE, ref.label='states', echo=TRUE}
```

To build the model we need transition probabilities and the state ids for the corresponding transitions. We here do this using a function:

```{r view_transPr, eval=FALSE, ref.label='transPr', echo=TRUE}
```

We can now build the model using the `binaryMDPWriter`:

```{r view_buldMDP1, eval=FALSE, ref.label='buildMDP1', echo=TRUE, tidy=FALSE}
```

Note that we build the model with two weights applied to each action "Duration" and "Net reward".
That is, when we specify and action we must add two weights. The process is built using first a
`process` which contains a `stage` (we only specify one stage, since we have a homogeneous semi-MDP
over an infinite horizon) which contains `state`s which contains `action`s. Transitions of an
`action` are specified using the `pr` and `id` parameter. The model is saved in a set of files with
prefix "`hct611-1_`".

The model can be loaded using 

```{r load}
mdp<-loadMDP("hct611-1_")
mdp # overall info
info<-infoMDP(mdp)  # more detailed info
info$actionDF
info$stateDF
```

Note the loaded model do not nessesary gives the same id to a state as when you built it, since the
order of the nodes in the hypergraph datastructure is optimized! Given the model in memory. We now
can find the optimal policy under various policies

```{r solve1}
# Optimal policy under average reward per time unit criterion
policyIteAve(mdp,"Net reward","Duration")
getPolicy(mdp)
# Optimal policy under expected discounted reward criterion (use both policy and value ite)
policyIteDiscount(mdp,"Net reward","Duration", discountFactor = 0.9)
getPolicy(mdp)
valueIte(mdp,"Net reward","Duration", discountFactor = 0.9, eps = 1e-10, maxIte = 1000)
getPolicy(mdp)
```

The model can also be built using by specifying a set of matrices. Note this way of specifying **only
work** for infinite-horizon semi-MDPs (and not finite-horizon or hierachical models). Specify a list
of probability matrices  (one for each action) where each row/state contains the transition
probablities, a matrix with rewards and a matrix with stage lengths (row = state, column = action).

```{r buildMDP2}
## define probability matrices
P<-list()
# a=1 (no repair)
P[[1]]<-as.matrix(rbind(Q,0))
# a=2 (preventive repair)
Z <- matrix(0, nrow = N, ncol = N)
Z[2,1]<-Z[3,1]<-Z[4,1]<-1
P[[2]]<-Z
# a=3 (forced repair)
Z <- matrix(0, nrow = N, ncol = N)
Z[5,1]<-1
P[[3]]<-Z
# reward 6x3 matrix with one column for each action
R <- matrix(0, nrow = N, ncol = 3)
R[2:4,2]<-Cp[2:4]
R[5,3]<-Cf
# state lengths
D <- matrix(1, nrow = N, ncol = 3)
D[5,3]<-2

# build model
w<-binaryMDPWriter("hct611-2_")
w$setWeights(c("Duration","Net reward"))
w$process(P,R,D)
w$closeWriter()
```

```{r solve2}
mdp<-loadMDP("hct611-2_")
policyIteAve(mdp,"Net reward","Duration")
getPolicy(mdp)
```




<!-- ## A finite Semi-MDP  -->

<!-- A *finite-horizon semi-MDP* considers a sequential decision problem over $N$ *stages*. Let $I_{n}$ -->
<!-- denote the finite set of system states at stage $n$. When *state* $i \in I_{n}$ is observed, an -->
<!-- *action* $a$ from the finite set of allowable actions $A_n(i)$ must be chosen, and this decision -->
<!-- generates *reward* $r_{n}(i,a)$. Moreover, let $\tau_n(i,a)$ denote the *stage length* of action -->
<!-- $a$, i.e. the expected time until the next decision epoch (stage $n+1$) given action $a$ and state -->
<!-- $i$. Finally, let $p_{ij}(a,n)$ denote the *transition probability* of obtaining state $j\in I_{n+1}$ -->
<!-- at stage $n+1$ given that action $a$ is chosen in state $i$ at stage $n$. A policy is a decision -->
<!-- rule/function that assigns to each state in a process an action. -->


<!-- A hypergraph -->
<!-- representing an MDP with time-horizon $N=5$ is shown in Figure 1. Each node corresponds to a -->
<!-- specific state in the MDP and a directed hyperarc is defined for each possible action. For instance, -->
<!-- node $v_{2,1}$ corresponds to a state number 1 at stage 2. The two hyperarcs with head in node -->
<!-- $v_{3,0}$ show that two actions are possible given state number 0 at stage 3. Action `mt` -->
<!-- corresponds to a deterministic transition to state number zero at stage 4 and action `nmt` -->
<!-- corresponds to a transition to state number 0 or 1 at stage 4 with a certain probability greater -->
<!-- than zero. -->

<!-- ![Figure 1: A state-expanded hypergraph for an MDP.](mdp_example_files/state_hgf.png) -->
<!-- *Figure 1: A state-expanded hypergraph for an MDP with time horizon $N=5$. At stage $n$ each node $v_{n,i}$ corresponds to a state in $\mathcal{S}_n$. The hyperarcs correspond to actions, e.g. if the system at stage $3$ is in state number 1 then there are two possible actions. Action `mt` results in a deterministic transition to state zero (because there is only one tail) at stage $4$ and `nmt` results in a transition to either state number 1 or 2 with a certain probability.* -->

<!-- States and actions can be identified using either an *unique id* or *state vector* -->
<!-- $\nu$. In an ordinary MDP the index vector consists of the stage and state -->
<!-- number, i.e. state corresponding to node $v_{3,1}$ in Figure 1 is -->
<!-- uniquely identified using $\nu=(n,s)=(3,1)$. Similar, action `buy` is -->
<!-- uniquely identified using $\nu=(n,s,a)=(0,0,0)$. Note that numbers in an **index always -->
<!-- start from zero**. -->

<!-- A hierarchical MDP is an MDP with parameters defined in a special way, but -->
<!-- nevertheless in accordance with all usual rules and conditions relating to such -->
<!-- processes (@Kristensen00). The basic idea of the hierarchical structure is -->
<!-- that stages of the process can be expanded to a so-called *child process*, -->
<!-- which again may expand stages further to new child processes leading to -->
<!-- multiple levels. To illustrate consider the MDP shown in Figure 2. The -->
<!-- process has three levels. At `Level 2` we have a set of ordinary MDPs -->
<!-- with finite time-horizon (one for each oval box) which all can be represented -->
<!-- using a state-expanded hypergraph (hyperarcs not shown, only hyperarcs -->
<!-- connecting processes are shown). An MDP at `Level 2` is uniquely defined -->
<!-- by a given state $s$ and action $a$ of its *parent process* at -->
<!-- `Level 1` (illustrated by the arcs with head and tail node at -->
<!-- `Level 1` and `Level 2`, respectively). Moreover, when a child -->
<!-- process at `Level 2` terminates a transition from a state $s\in -->
<!-- \mathcal{S}_{N}$ of the child process to a state at the next stage of the -->
<!-- parent process occur (illustrated by the (hyper)arcs having head and tail at -->
<!-- `Level 2` and `Level 1`, respectively). Since a child process is -->
<!-- always defined by a stage, state and action of the parent process we have that -->
<!-- for instance a state at level 1 have an index vector -->
<!-- $\nu=(n_{0},s_{0},a_{0},n_{1},s_{1})$. -->

<!-- ![Figure 2: A state-expanded hypergraph for an HMDP.](mdp_example_files/hmdp_index.png) -->
<!-- *Figure 2: A hypergraph representation of the first stage of a hierarchical MDP. Level 0 indicate the -->
<!-- founder level, and the nodes indicates states at the different levels. A child -->
<!-- process (oval box) is represented using its state-expanded hypergraph (hyperarcs -->
<!-- not shown) and is uniquely defined by a given state and action of its parent -->
<!-- process.* -->

<!-- In general a state $s$ and action $a$ at level $l$ can be uniquely identified -->
<!-- using -->
<!-- $$ -->
<!-- \begin{aligned} -->
<!-- \nu_{s}&=(n_{0},s_{0},a_{0},n_{1},s_{1},\ldots,n_{l},s_{l}) \\ -->
<!-- \nu_{a}&=(n_{0},s_{0},a_{0},n_{1},s_{1},\ldots,n_{l},s_{l},a_{l}). -->
<!-- \end{aligned} -->
<!-- $$ -->
<!-- The index vector of three states is illustrated in Figure 2. -->

<!-- Another way to identify a state or action is using an id number. Id numbers can -->
<!-- be seen when printing information about the model i R. This will be further -->
<!-- clarified in the example in the next section. -->





























<!-- ## Figures -->

<!-- The figure sizes have been customised so that you can easily put two images side-by-side.  -->

<!-- ```{r, fig.show='hold'} -->
<!-- plot(1:10) -->
<!-- plot(10:1) -->
<!-- ``` -->

<!-- You can enable figure captions by `fig_caption: yes` in YAML: -->

<!--     output: -->
<!--       rmarkdown::html_vignette: -->
<!--         fig_caption: yes -->

<!-- Then you can use the chunk option `fig.cap = "Your figure caption."` in **knitr**. -->

<!-- ## More Examples -->

<!-- You can write math expressions, e.g. $Y = X\beta + \epsilon$, footnotes^[A footnote here.], and tables, e.g. using `knitr::kable()`. -->

<!-- ```{r, echo=FALSE, results='asis'} -->
<!-- knitr::kable(head(mtcars, 10)) -->
<!-- ``` -->

<!-- Also a quote using `>`: -->

<!-- > "He who gives up [code] safety for [code] speed deserves neither." -->
<!-- ([via](https://twitter.com/hadleywickham/status/504368538874703872)) -->


## References
