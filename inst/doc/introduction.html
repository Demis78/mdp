<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1">

<meta name="author" content="Lars Relund lars@relund.dk" />

<meta name="date" content="2016-04-01" />

<title>An introduction to the MDP package in R</title>



<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>


<link href="data:text/css;charset=utf-8,body%20%7B%0Abackground%2Dcolor%3A%20%23fff%3B%0Amargin%3A%201em%20auto%3B%0Amax%2Dwidth%3A%20700px%3B%0Aoverflow%3A%20visible%3B%0Apadding%2Dleft%3A%202em%3B%0Apadding%2Dright%3A%202em%3B%0Afont%2Dfamily%3A%20%22Open%20Sans%22%2C%20%22Helvetica%20Neue%22%2C%20Helvetica%2C%20Arial%2C%20sans%2Dserif%3B%0Afont%2Dsize%3A%2014px%3B%0Aline%2Dheight%3A%201%2E35%3B%0A%7D%0A%23header%20%7B%0Atext%2Dalign%3A%20center%3B%0A%7D%0A%23TOC%20%7B%0Aclear%3A%20both%3B%0Amargin%3A%200%200%2010px%2010px%3B%0Apadding%3A%204px%3B%0Awidth%3A%20400px%3B%0Aborder%3A%201px%20solid%20%23CCCCCC%3B%0Aborder%2Dradius%3A%205px%3B%0Abackground%2Dcolor%3A%20%23f6f6f6%3B%0Afont%2Dsize%3A%2013px%3B%0Aline%2Dheight%3A%201%2E3%3B%0A%7D%0A%23TOC%20%2Etoctitle%20%7B%0Afont%2Dweight%3A%20bold%3B%0Afont%2Dsize%3A%2015px%3B%0Amargin%2Dleft%3A%205px%3B%0A%7D%0A%23TOC%20ul%20%7B%0Apadding%2Dleft%3A%2040px%3B%0Amargin%2Dleft%3A%20%2D1%2E5em%3B%0Amargin%2Dtop%3A%205px%3B%0Amargin%2Dbottom%3A%205px%3B%0A%7D%0A%23TOC%20ul%20ul%20%7B%0Amargin%2Dleft%3A%20%2D2em%3B%0A%7D%0A%23TOC%20li%20%7B%0Aline%2Dheight%3A%2016px%3B%0A%7D%0Atable%20%7B%0Amargin%3A%201em%20auto%3B%0Aborder%2Dwidth%3A%201px%3B%0Aborder%2Dcolor%3A%20%23DDDDDD%3B%0Aborder%2Dstyle%3A%20outset%3B%0Aborder%2Dcollapse%3A%20collapse%3B%0A%7D%0Atable%20th%20%7B%0Aborder%2Dwidth%3A%202px%3B%0Apadding%3A%205px%3B%0Aborder%2Dstyle%3A%20inset%3B%0A%7D%0Atable%20td%20%7B%0Aborder%2Dwidth%3A%201px%3B%0Aborder%2Dstyle%3A%20inset%3B%0Aline%2Dheight%3A%2018px%3B%0Apadding%3A%205px%205px%3B%0A%7D%0Atable%2C%20table%20th%2C%20table%20td%20%7B%0Aborder%2Dleft%2Dstyle%3A%20none%3B%0Aborder%2Dright%2Dstyle%3A%20none%3B%0A%7D%0Atable%20thead%2C%20table%20tr%2Eeven%20%7B%0Abackground%2Dcolor%3A%20%23f7f7f7%3B%0A%7D%0Ap%20%7B%0Amargin%3A%200%2E5em%200%3B%0A%7D%0Ablockquote%20%7B%0Abackground%2Dcolor%3A%20%23f6f6f6%3B%0Apadding%3A%200%2E25em%200%2E75em%3B%0A%7D%0Ahr%20%7B%0Aborder%2Dstyle%3A%20solid%3B%0Aborder%3A%20none%3B%0Aborder%2Dtop%3A%201px%20solid%20%23777%3B%0Amargin%3A%2028px%200%3B%0A%7D%0Adl%20%7B%0Amargin%2Dleft%3A%200%3B%0A%7D%0Adl%20dd%20%7B%0Amargin%2Dbottom%3A%2013px%3B%0Amargin%2Dleft%3A%2013px%3B%0A%7D%0Adl%20dt%20%7B%0Afont%2Dweight%3A%20bold%3B%0A%7D%0Aul%20%7B%0Amargin%2Dtop%3A%200%3B%0A%7D%0Aul%20li%20%7B%0Alist%2Dstyle%3A%20circle%20outside%3B%0A%7D%0Aul%20ul%20%7B%0Amargin%2Dbottom%3A%200%3B%0A%7D%0Apre%2C%20code%20%7B%0Abackground%2Dcolor%3A%20%23f7f7f7%3B%0Aborder%2Dradius%3A%203px%3B%0Acolor%3A%20%23333%3B%0Awhite%2Dspace%3A%20pre%2Dwrap%3B%20%0A%7D%0Apre%20%7B%0Aborder%2Dradius%3A%203px%3B%0Amargin%3A%205px%200px%2010px%200px%3B%0Apadding%3A%2010px%3B%0A%7D%0Apre%3Anot%28%5Bclass%5D%29%20%7B%0Abackground%2Dcolor%3A%20%23f7f7f7%3B%0A%7D%0Acode%20%7B%0Afont%2Dfamily%3A%20Consolas%2C%20Monaco%2C%20%27Courier%20New%27%2C%20monospace%3B%0Afont%2Dsize%3A%2085%25%3B%0A%7D%0Ap%20%3E%20code%2C%20li%20%3E%20code%20%7B%0Apadding%3A%202px%200px%3B%0A%7D%0Adiv%2Efigure%20%7B%0Atext%2Dalign%3A%20center%3B%0A%7D%0Aimg%20%7B%0Abackground%2Dcolor%3A%20%23FFFFFF%3B%0Apadding%3A%202px%3B%0Aborder%3A%201px%20solid%20%23DDDDDD%3B%0Aborder%2Dradius%3A%203px%3B%0Aborder%3A%201px%20solid%20%23CCCCCC%3B%0Amargin%3A%200%205px%3B%0A%7D%0Ah1%20%7B%0Amargin%2Dtop%3A%200%3B%0Afont%2Dsize%3A%2035px%3B%0Aline%2Dheight%3A%2040px%3B%0A%7D%0Ah2%20%7B%0Aborder%2Dbottom%3A%204px%20solid%20%23f7f7f7%3B%0Apadding%2Dtop%3A%2010px%3B%0Apadding%2Dbottom%3A%202px%3B%0Afont%2Dsize%3A%20145%25%3B%0A%7D%0Ah3%20%7B%0Aborder%2Dbottom%3A%202px%20solid%20%23f7f7f7%3B%0Apadding%2Dtop%3A%2010px%3B%0Afont%2Dsize%3A%20120%25%3B%0A%7D%0Ah4%20%7B%0Aborder%2Dbottom%3A%201px%20solid%20%23f7f7f7%3B%0Amargin%2Dleft%3A%208px%3B%0Afont%2Dsize%3A%20105%25%3B%0A%7D%0Ah5%2C%20h6%20%7B%0Aborder%2Dbottom%3A%201px%20solid%20%23ccc%3B%0Afont%2Dsize%3A%20105%25%3B%0A%7D%0Aa%20%7B%0Acolor%3A%20%230033dd%3B%0Atext%2Ddecoration%3A%20none%3B%0A%7D%0Aa%3Ahover%20%7B%0Acolor%3A%20%236666ff%3B%20%7D%0Aa%3Avisited%20%7B%0Acolor%3A%20%23800080%3B%20%7D%0Aa%3Avisited%3Ahover%20%7B%0Acolor%3A%20%23BB00BB%3B%20%7D%0Aa%5Bhref%5E%3D%22http%3A%22%5D%20%7B%0Atext%2Ddecoration%3A%20underline%3B%20%7D%0Aa%5Bhref%5E%3D%22https%3A%22%5D%20%7B%0Atext%2Ddecoration%3A%20underline%3B%20%7D%0A%0Acode%20%3E%20span%2Ekw%20%7B%20color%3A%20%23555%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%0Acode%20%3E%20span%2Edt%20%7B%20color%3A%20%23902000%3B%20%7D%20%0Acode%20%3E%20span%2Edv%20%7B%20color%3A%20%2340a070%3B%20%7D%20%0Acode%20%3E%20span%2Ebn%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Efl%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Ech%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Est%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Eco%20%7B%20color%3A%20%23888888%3B%20font%2Dstyle%3A%20italic%3B%20%7D%20%0Acode%20%3E%20span%2Eot%20%7B%20color%3A%20%23007020%3B%20%7D%20%0Acode%20%3E%20span%2Eal%20%7B%20color%3A%20%23ff0000%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%0Acode%20%3E%20span%2Efu%20%7B%20color%3A%20%23900%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%20code%20%3E%20span%2Eer%20%7B%20color%3A%20%23a61717%3B%20background%2Dcolor%3A%20%23e3d2d2%3B%20%7D%20%0A" rel="stylesheet" type="text/css" />

</head>

<body>



<div class="fluid-row" id="header">


<h1 class="title">An introduction to the MDP package in R</h1>
<h4 class="author"><em>Lars Relund <a href="mailto:lars@relund.dk">lars@relund.dk</a></em></h4>
<h4 class="date"><em>2016-04-01</em></h4>

</div>


<style> 
p {text-align: justify;} 
//.sourceCode {background-color: white;}
pre {
  border-style: solid;
  border-width: 1px;
  border-color: grey;
  //background-color: grey !important;
}
</style>
<!-- scale math down -->
<script type="text/x-mathjax-config"> 
    MathJax.Hub.Config({ 
        "HTML-CSS": { scale: 80 }
        });
</script>
<p>The MDP package in R is a package for solving Markov decision processes (MDPs) with discrete time-steps, states and actions. Both traditional MDPs <span class="citation">(Puterman 1994)</span>, semi-Markov decision processes (semi-MDPs) <span class="citation">(Tijms 2003)</span> and hierarchial-MDPs (HMDPs) <span class="citation">(A. R. Kristensen and Jørgensen 2000)</span> can be solved under a finite and infinite time-horizon. <!-- In this paper we use the abbreviation MDP for all types. --></p>
<p>Building and solving an MDP is done in two steps. First, the MDP is built and saved in a set of binary files. Next, you load the MDP into memory from the binary files and apply various algorithms to the model.</p>
<p>In the package are implemented well-known algorithms such as policy iteration and value iteration under different critria e.g. average reward per time unit and expected total discounted reward. The model is stored using an underlying data structure based on the <em>state-expanded directed hypergraph</em> of the MDP (<span class="citation">Nielsen and Kristensen (2006)</span>) implemented in <code>C++</code> for fast running times. <!-- Under development is also
support for MLHMP which is a Java implementation of algorithms for solving MDPs (@Kristensen03). 
--></p>
<p>The newest version of the package can be installed from GitHub</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">&gt;<span class="st"> </span><span class="kw">install_github</span>(<span class="st">&quot;relund/mdp&quot;</span>)</code></pre></div>
<p>We load the package using</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">&gt;<span class="st"> </span><span class="kw">library</span>(MDP2)</code></pre></div>
<p>Help about the package can be seen by writing</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">&gt;<span class="st"> `</span><span class="dt">?</span><span class="st">`</span>(MDP2)</code></pre></div>
<p>To illustrate the package capabilities, we use two examples, namely, a semi-MDP and a HMDP. Before each example a short introduction to these two models are given.</p>
<div id="an-infinite-semi-mdp" class="section level2">
<h2>An infinite Semi-MDP</h2>
<p>An <em>infinite-horizon semi-MDP</em> considers a sequential decision problem over an infinite number of <em>stages</em>. Let <span class="math inline">\(I\)</span> denote the finite set of system states at stage <span class="math inline">\(n\)</span>. Note we assume that the semi-MDP is <em>homogeneous</em>, i.e the state space is independent of stage number. When <em>state</em> <span class="math inline">\(i \in I\)</span> is observed, an <em>action</em> <span class="math inline">\(a\)</span> from the finite set of allowable actions <span class="math inline">\(A(i)\)</span> must be chosen, and this decision generates <em>reward</em> <span class="math inline">\(r(i,a)\)</span>. Moreover, let <span class="math inline">\(\tau(i,a)\)</span> denote the <em>stage length</em> of action <span class="math inline">\(a\)</span>, i.e. the expected time until the next decision epoch (stage <span class="math inline">\(n+1\)</span>) given action <span class="math inline">\(a\)</span> and state <span class="math inline">\(i\)</span>. Finally, let <span class="math inline">\(p_{ij}(a)\)</span> denote the <em>transition probability</em> of obtaining state <span class="math inline">\(j\in I\)</span> at stage <span class="math inline">\(n+1\)</span> given that action <span class="math inline">\(a\)</span> is chosen in state <span class="math inline">\(i\)</span> at stage <span class="math inline">\(n\)</span>. A policy is a decision rule/function that assigns to each state in the process an action.</p>
<p>We consider example 6.1.1 in <span class="citation">Tijms (2003)</span>. At the beginning of each day a piece of equipment is inspected to reveal its actual working condition. The equipment will be found in one of the working conditions <span class="math inline">\(i = 1,\ldots, N\)</span> where the working condition <span class="math inline">\(i\)</span> is better than the working condition <span class="math inline">\(i+1\)</span>. The equipment deteriorates in time. If the present working condition is <span class="math inline">\(i\)</span> and no repair is done, then at the beginning of the next day the equipment has working condition <span class="math inline">\(j\)</span> with probability <span class="math inline">\(q_{ij}\)</span>. It is assumed that <span class="math inline">\(q_{ij}=0\)</span> for <span class="math inline">\(j&lt;i\)</span> and <span class="math inline">\(\sum_{j\geq i}q_{ij}=1\)</span>. The working condition <span class="math inline">\(i=N\)</span> represents a malfunction that requires an enforced repair taking two days. For the intermediate states <span class="math inline">\(i\)</span> with <span class="math inline">\(1&lt;i&lt;N\)</span> there is a choice between preventively repairing the equipment and letting the equipment operate for the present day. A preventive repair takes only one day. A repaired system has the working condition <span class="math inline">\(i=1\)</span>. The cost of an enforced repair upon failure is <span class="math inline">\(C_{f}\)</span> and the cost of a preemptive repair in working condition <span class="math inline">\(i\)</span> is <span class="math inline">\(C_{p}(i)\)</span>. We wish to determine a maintenance rule which minimizes the long-run average repair cost per day.</p>
<p>To formulate this problem as an infinite horizon semi-MDP the set of possible states of the system is chosen as <span class="math display">\[
I=\{1,2,\ldots,N\}.
\]</span> State <span class="math inline">\(i\)</span> corresponds to the situation in which an inspection reveals working condition <span class="math inline">\(i\)</span>. Define actions <span class="math display">\[
a=\left\{\begin{array}{ll}
0 &amp; \text{if no repair.}\\
1 &amp; \text{if preventive repair.}\\
2 &amp; \text{if forced repair.}\\
\end{array}\right.
\]</span> The set of possible actions in state <span class="math inline">\(i\)</span> is chosen as <span class="math inline">\(A(1)=\{0\},\ A(i)=\{0,1\}\)</span> for <span class="math inline">\(1&lt;i&lt;N, A(N)=\{2\}\)</span>. The one-step transition probabilities <span class="math inline">\(p_{ij}(a)\)</span> are given by <span class="math inline">\(p_{ij}(0) = q_{ij}\)</span> for <span class="math inline">\(1\leq i&lt;N\)</span>, <span class="math inline">\(p_{i1}(1) = 1\)</span> for <span class="math inline">\(1&lt;i&lt;N\)</span>, and zero otherwise. The one-step costs <span class="math inline">\(c_{i}(a)\)</span> are given by <span class="math inline">\(c_{i}(0)=0,\ c_{i}(1)=C_{p}(i)\)</span> and <span class="math inline">\(c_{N}(2)=C_{f}\)</span>. The stage length until next decision epoch are <span class="math inline">\(\tau_i(a) = 1, 0\leq i &lt; N\)</span> and <span class="math inline">\(\tau_N(a) = 2\)</span>.</p>
<p>Assume that the number of possible working conditions equals <span class="math inline">\(N=5\)</span>. The repair costs are given by <span class="math inline">\(C_{f}=10,\ C_{p}(2)=7,\ C_{p}(3)=7\)</span> and <span class="math inline">\(C_{p}(4)=5\)</span>. The deterioration probabilities <span class="math inline">\(q_{ij}\)</span> are given by</p>
<table>
<thead>
<tr class="header">
<th align="left"></th>
<th align="right">1</th>
<th align="right">2</th>
<th align="right">3</th>
<th align="right">4</th>
<th align="right">5</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">1</td>
<td align="right">0.9</td>
<td align="right">0.1</td>
<td align="right">0.0</td>
<td align="right">0.00</td>
<td align="right">0.00</td>
</tr>
<tr class="even">
<td align="left">2</td>
<td align="right">0.0</td>
<td align="right">0.8</td>
<td align="right">0.1</td>
<td align="right">0.05</td>
<td align="right">0.05</td>
</tr>
<tr class="odd">
<td align="left">3</td>
<td align="right">0.0</td>
<td align="right">0.0</td>
<td align="right">0.7</td>
<td align="right">0.10</td>
<td align="right">0.20</td>
</tr>
<tr class="even">
<td align="left">4</td>
<td align="right">0.0</td>
<td align="right">0.0</td>
<td align="right">0.0</td>
<td align="right">0.50</td>
<td align="right">0.50</td>
</tr>
</tbody>
</table>
<p>A state-expanded hypergraph representing the MDP with infinite time-horizon is shown below. Each node corresponds to a specific state in the MDP and is given an <em>unique id</em> (<strong>id must always start from zero</strong>). A directed hyperarc is defined for each possible action. For instance, the state/node with id 1 corresponding to working condition <span class="math inline">\(i=2\)</span> and the two hyperarcs with head in this node corresponds to the two actions preventive and no repair. Note the tails of a hyperare represent a possible transition (<span class="math inline">\(p_{ij}(a)&gt;0\)</span>).</p>
<p><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAA8AAAAHgCAMAAABdO/S2AAAAmVBMVEUAAAAAACgAADoAAGYAKJAAOpAAZrYAms0XAAAoAAAoZpAogf86AAA6ADo6AGY6Ojo6kNtFiwBmAABmADpmWgBmZmZmkJBmtrZmtv98ezqQOgCQZgCQkGaQtpCQ2/+2ZgC2fwC2tma225C2/7a2///bRwDbkCjbkDrb/7bb////WgD/bQD/fwD/tmb/22b/25D//7b//9v////Zy7RgAAAACXBIWXMAAA7DAAAOwwHHb6hkAAAgAElEQVR4nO2di5rbtpZmOa4ppy9yT5Wck2jcSaknxzM67i61ynr/hxsCICneicsGsDf4r++LUwJBgAKxtEESJKs7AEAsVe4NAAD4A4EBEAwEBkAwEBgAwUBgAAQDgQEQDAQGQDAQGADBQGAABOMkcDUk1jYBsC8CxLLLPFsyNAYgjHCxtrNtFQaLAfCBRKyNHJZywmEAXCATa3Wxi5ZQGABLHMVajdMklbRrOK4AwA5xNmst/+IiLxuhMADrEIu1sMDXRAykAVjBV5DF9eaTAzSEwQAsQS/WbGqQhDAYgHkiiDWXGKggDAZgjhhizaQFCwiDAZgSRaztpGtVfXp7fPw4nobLb//23aJQAPZOFLGmSeNq6kqu/YpGfByfpvXAYABGjKwgEmsr5ee3l/rf8+elauqfkWk9CMEAjIkj1lbK7aAC++Xp+89vpq5hpL9WL1cIDMA2ccTaFPiLCvK9slQ9t4O+U8LEfwgMgAVxxNpKMaP03lh9cqwNgQGwII5YkQR+V0ySAdglWoc8As9Gest63kdMsgFQJLM9n06sjZRBUnes3a9na6i+MIKG0KBMLHr2L7/8QiCWjcDDpMnZbosfCtvLwBAayMSh5/7ScB95QSXW5vTK8fXm7Xq8p3FAaMATj57ZE7cjUKxtWdu0QeLFhPT5y1Uz9dBNw4LPIA8BoWRO3I6BG65iLdw5uF2RG/GmUSJAg0gQdK1Vcbscie4HDtAw4TRoCA18oes62+K2ufQf5GItlef36I+sT9SB0GAJ+r5hJ26b8/GJWizap1KymoEFofdLtH3/i724bfbhZ2qxNh4NT1QLCyB0uUTft27edquME/T/XynFWi/J+o0LEl/NAJ/lkuzH2EPcdrVJivnj9fVOKdZmMdtvaCnk5Ug7CdCTqxUiSL5zPMVtV52mNX9ofTUW2tiIZWXe4tvSSn4/4U6E5km2xg8Qt119LrH566GvgUAse/uqOazXlg+Ejkj2xg0Uty1iNrX9c6xvS5BYe1KQlux9rh4N/36sKjUivta7/KVJ/PqHehbLxaTcvvx5aBadK51XDaGbPBnJ33gNBOK2xcynd38u6RsIBKYifZ/8OH56u1/UlNrazI9jOyPv+cddp94OL+r2lpNerKfNq1QtsM6TFC7CthCJ2xa1tKD7O5K+EDge8YX+OL7o29LMjS3NxHidqP9Rk2mVxLW4zz8+vr6ZW9iMwC/0WzOETYgdQShuW9zikseHaPpC4HTQ92h9Qrn+x9xaav41iUbmOsUkNm5fq6oVmPxMNFdhW4jFbYtcXtb7EFFfCJwNgh4/FLix0gjcnAo5Nc99UEPtqnr660AmMHdhGxwnTjmVu7aw9ymqvhCYDY5GqOVbEfjePbjl05teevMXWIiwLZG87cpeXdr/GFlfCMyWdWPe+wKPjoFPvdka5hj4/PxD31x6tR9CCxO2Jaa4bfnriwefo+sLgcUwNqp2qnN1eBZaP2tJ2XxWgbc5SW2Cb/WyJLBQYVtii9vWsZFh+DmBvhBYJo1lx1+bf/6zdx3YyKmuA9dB93b47WAuFevnP9RKNwILF7YlhbhtPZs5hglJ9IXAUpkYN29kc2AsPsKOSSVuW9d2llFKIn0hcLEYT//r8GsRwrakFLetzyLTOCWZvhC4cFQELsHg1OK2dVrlGicl1BcC7waZFucQt63XLtskLam+EHhfiLE4l7ht3ZYZp2mJ9YXAO4TzkDqnuG391jmnicn1hcC7hZXFsWY8Om+EfdaZ1Az6QuB9k9tiDt52G+KQeS41i74QGOQYUrMRV+G2IQuZM+kLgUFDEotZiatw3Jil3Nn0hcCgTyyL2YmrcN6gpewZ9YXAYALhkJqluAr3jVpcIau+EBgsEGQxW3EVHhu2vEZmfSEwWMM1GLMWV+G1ccurZNcXAoNtti1mL67CbwNXVmKgLwQGlsxaLEJchedGrq3FQl8IDFxohtQsJk5Z472ha6sx0RcCAwdabznNwlzH/1dmdUU2+kJgYMFcwM09C3ObgCHC+pqM9IXAYIXNkTLbG5uCxvfrq7LSFwKDGRwPcZlZHHZwvrEyM30hMCCCh8WBZ9a21manLwQGlGQdUoeeFt9cnaG+EHh3XP1fjWS7XgaLw69pba7PUl8IvDcivJhwnnTBmOCC9HYJTPWFwHsjmcCG2BZTzCaxKIKtvhBYPLcvfx70e1U+vv6hXqZy0W9ZebwX2Hyuxf39qF43Wud9/sfxNFmuGJTwKPh+P1f69Sz6rSwmjxORLKaZCmZRBmN9IbB4aiXbt5s9/zBvNbsdXn5+U686Oz//aD7XS5Ws3auRJssVgxIeBd/Pn02qWVfl8YB2SE00j9OmFNb6QmDxGPsuzz8+jjrQ6jeNPn2/1GGy/tB91n/Ucbd9OeF4uSpqWMKj4K9v/VcLv4RsLYXFVJOwrYphri8EFk83GO692bsRtf4w+KwMbAUeL1dFDUt4jLLV/yr7VwtvEmAx3R0UVuWw1xcCS6XrfrcvyrBO4MpwUuNe9V7v9vNY4PFyVdSwhK5gdVD89NeBTmCN+5Ca8PYnu5IE6AuBhWI6oOrSI4FNwFQpT383QdZ8nAg8Wt7l6VK6gnUovlELbLC1mPLeRcuiROgLgUXSn6tsDlXP6hj41L9K9HH8mz7MPbUfRwKPlvfzmE9dwfoI+Uo4hB6zYTHtjceWZQnRFwKL4pcebdrtUIfMVi9ztvh+1v9Un3ufHwK/NDmHy1VRwxK6gk3wrV6iCayZH1ITPzXAtjQx+kJgEUzvDuoL/Nuhu0pbo67i6pPK16r/uYuuZ30deLJcMSyhK1glfHo7d5egYn7TvsXUj/ywLk6QvhCYNSu39fUEjmRUtII3qR2mf16PdXmi9IXALHG5H7c8gZtvTjnxw/7nQJi+EJgVPs+KK0vgGA/ucWhQcfpCYBb4iFseiy0QNAvToVUF6guBswJxWyxawcdil7YVqS8EzgLE7ePSEi4WOzWwUH0hcFog7giv1rAaUjuVK1ZfCJwGhNwZAhtkzWK3kgXrC4HjAnEXoGqUOYsdixatLwSOA8Rdgbxh+kNq17KF6wuBaYG4G0RsnHc9gctplVfx+kJgGiCuBbEbSBXvcpa6AHvvEDgMiGtJ9Ebql291lroMfSGwHxDXgQQNNVfBqsWl6AuB3YC4jqRorLUq5oNxOfpCYDsgrgdJGsyqjqHFJekLgdeBuJ4kajSXShqLy9IXAs8DcQNI1XA+1by+sngLKiEQuA/EDSRZ43nV00ZfHu8ypgECayBuOAkb0Kui0eC5EIt3LjBCLg0p29Cvqvlj36xvJCdhpwJDXDqStqNnXeunriRbvDOBIS4tidvSszKrM89CLd6JwBCXntTt6Vudy4UjeUPqwgWGuHFI3qbe9flc95VkcaECQ9x4ZGhX7woDpm0ICcaFCQxx45Kjbf2rJJh1xd7iQgSGuPHJ0r4BddJNmuRssXCBIW4aMrVxQKXkc56ZDqmFCgxx05GrnUOqjXbLAjuL7QWuZoi4YfNA3LRka+ugemPfcURr8ZxYDlraV+GygBqIm5587R1WcaIbBimG1ARibWbaLiimxBA3DznbPKzmxPf7+lu8rY2VWNtFWG+NVUZbIG42srZ7YN15btd3t5hOrLWlblLSKIyQm5W8TR9aedanbTgMqSnFWl7mLmSIwhA3O7mbP7R2Fg/LsbCYVqylJX4yuq8FcVmQfRcE189C34a1YEwt1kK6dyy1XRHisiH/bgjfAE76tsxaTC7WfHLAWHhjVYjLCg67InwLOOrbMLI45DzR/Lr2ifYVzSVCXHaw2B0E28BY34ZuSB12nnf+irFlmlNFvb8hLkt47BKKjeCvb8v7e+h1GktZJ9Xc/u3748PH8dRb9PNbfXj9MlMAxGULl91CsRVy9L0TiTUpdLuaj+PT90mutppPb/dL9XlcBI8eAiZwkZck+MrSd+qah1gzum6mXKtqsZ7bQf1qXMbLM9zlADZhIy+NvdL0pRDLQtdJyrV6uapyfn4zPwfDSG+y1L8WW6WCrPCRl2boLE/fWGLZpFyfRkP120HPs26LP1v8UACgIPohkadvLLG8BB4tnhxsQ2AwB9UwQKK+scQKF/g6OdSGwIy5nuYGa1b4rtdANYiXqW8ssYIFnvmZgMB8CZTQG7JjcKn6xhLL4jLS6lD9MlsNBOZKFoHpzqDJ1ZdELCuBp0krPxSXaq4/wN903L78edCX/D++/qEuS1z0BABzGeKqLyaapcffj1V1Uj3k+R/H02S5YlDCo+D7/VynnMyub/K4QHf+W7K+90hi2czEWq7ndpj9mYDA6aiVPNXDLXVk+/yj3vG1k/VeMdcmzs8/ms/1UiXrpzct4fE0Wa4YlPAo+H7+bFLNuiqPPYSXr4TrG0ms+SdqzdQze7nqUg1Oe7er83rwZtmYXX15/vFx1IFWfap3mJoDUH/oPus/6rjbCHwfL1dFDUt4FPz1zcwsMOvO9awFKC8+i9d3Zi60s1gzpc7fjRQSQvXKTJ+CXSDdYFh3AHPpvxG1/jD4rPpIK/B4uSpqWMJjlH3Xs4Ye69lBOnVEtr40dyM53E5IdT8wLI5F40Ytye2LMqwTuHms8EmNe+sRcvd5LPB4uSptWEJXsAoHT38d3AQmnfglWF+6+4EXViV+IsfMoz9gcQQe93qNBO4GXdenv5sgaz5OBB4t7/J0KV3BOhTfXASmnbcpVd+FJ3J4m7WQ7rqCVy0YUtPSm9psDlXP6hj41D+Q+jj+TR/mntqPI4FHy/t5RgXrI7Wr9RCaeNa1RH1XezutWMRPpdzIAIsJGN1ofTvUIbPVy5wtvp/1P3oiT/v5IfBLk3O4XBU1LKEr2ATf6sVKYOJ7JsTpa9PFkzyVcnNF77yw2J+Hub0I/Nuhu0p7N+cv9Unla9X/3EXXs74OPFmuGJbQFawSPr2du0tQq5tH+nVF6esyyCQUa6MgyyfIO7+YAUNqZxbuCGzOINPjWjD5DYty9PXozGRibZey9YaWgFcjwWJLVm7nZSIw+e3GQvQN6MI0Ytmp11xaqLYT3YHF62zci89BYPqnBQjQl2QQGS6Wk37VEJdVN8CQehZOD9JYIsImcteXvLMGiMVq2jIs7iFB3ghDZ+b6suuirATWsGuiDMiQN0bw5asv00EiP4E1TFsrBULkjWIvU305d0amAhs4N1wUpMgbZejMUl/2XZC1wJq9BGM58sYJvtz0FdLv+AtskNGavgiSN5K9rPSV1NmkCKyR1LDWSJI30tCZj75Cwm4PUQJr5LXxMrLkjRV8eegrtFvJE9ggtLl7CJM3mr0M9JXcmaQKrJHa8AJfvBpte/PqK384J1pgjbB9IM/diME3p76yus0i8gU2iNgdEuWNaG82fUV0FktKEVjDeceIlDfi0DmPvsKGaxYUJbCG4T4SKm/M4JteX37dgoTyBDaw2V1S5Y1qb2J92XSGCJQqsCb3jhMrb1x7E+rLcDhGTNECazLtQ7nyRj3wvSfTt3h1DeULbEi6OyXLGzn4ptC3/LDbYy8Ca1LsWNHyxrc3sr57UtewK4E1EX+fZcsbe+gcOfjuz13N/gQ2kO9u6fLGDr4R9d3VkHnMXgXWUO148fJGtzeWvntW17BrgTWBv9/y5Y0+dI6jL9zVQGCDV3coQd74wZdc310PmcdA4B4uPaMIeRPYS6sv1B0DgSdsd5Iy5E0wdKbUF+7OAoHnWewupcibIvgS6Ysh8woQeIVxzylG3iT2UugLdbeAwJuYTlSOvEmGzuH6wl0rILAFSt5iulOaH6IQfTFkdgACb9CPvPJ7VqJhhLe+4hs4ORB4hflhs9xOluggwEtf+T+OeYDAC2wc88rrbqmO4d31ldeWjIDAM9iesJITNZKdgXPUV0wDsgUCj3A/28y/EyY7f+6gr5wfP95A4B4Bl4r4dsd0l79s9eXbVgKBwAaSt53wiyoJL15b6cuugcQDgcknWLHppCmnnmzqy+/HrQz2LnCsCVb5u2vKiWPr+uZvi4LZs8DRZ0fmizpJ532u6At3Y7NXgRNObS68Ey/oiyFzGvYocI77EkrtznP6lvpdWbI3gbPeVMQhKl1P94/jyWvV6XpjfTl8wZ2xJ4GZ3BGYtZP7yjvHQF+om4e9CMxE3o5csYpO4Ie+cDcjexCYm7wPCLr+7cufh6p6qdX8+kf19P1+qdSn20F5ev301nyuxf39WFWnW533+R/H02S5YlDCo+D7/VynnIz8TZ5GXwyZs1O6wHzl7QiToFbydL9W6sj2+cf9fqmdvB1efn77XC87P/9oPtdLlayf3rSEx9NkuWJQwqPg+/mzSTXrqjy1vlCXByULLEDeDu9YZuy7PP/4OOpAqz5dn75f6jBZf+g+6z/quNsIfB8vV0UNS3gU/PVNr9oI/FLrC3fZUKrAkuR94GZxPSz+5ZduMKyPbtWguBO1/jD4rAxsBR4vN+X1S3iMstX/qqpdL93buYEFJQosU94OG4v196u/Zj3a/aIM6wSuDCc17q1HyN3nscDj5aq8YQldweqg+OmvJgK/vtKdxQYElCawcHk7NobUv6hvaobFQ4FNwFQpT383QdZ8nAg8Wt7l6VK6gnUoNkPo11fKy1CAgJIELkXeB4sW66+qZTKHqmd1DHzqXyX6OP5NH+ae2o8jgUfL+3nMp65gfYSszmap6AuBmVGKwOXJ2zGxuL13uRG4DpnXqlVLnS2+n/U/1efe54fAL03O4fJ7W16X0hVsgm9loi8EZkYJAhcsb8djSP1LQyPcf//Tvxz0Mex//9O/qsX/rz5+ffo/jXv1OPh/15//x6c3vVT9c67+5z+O/3wyMbVGXfU1J6FbOf+vOgT+j9rb30zBKsun19fuElSeBgDzSBd4D/I+eG+eMG9cbgT+15/f9JWfXhhdGQevWtgP9s2paQ3OPLNFssD7kre7yXcicCPmZ51pfRz8sijwZKD+EBj6MkaqwHuTd/SEDSNxK7C+9tPOhzz2R8bDcfDbeW4cvHDCuxUY+rJGosD7k3fyhI1GuM1Lxv1x8Azbl5yhL3OkCbxDebeej7Nm4bLAVpO+oC97JAm8R3ltH063MQ62yDoF+gpAisD7lNf14XRbarrMtYa+IpAg8E7l9X205Lyljnc7QV8hMBeY5IUJMgn73r1xsseditBXDIwF3q+7ZM91fn/gsBb0FQRTgfcsL+1T2Vt530csrgB9RcFQ4F3LG+GVKPPCjoVuMkBfYTATeOfyRnolirJzcxStJNYPy3Efc4N8MBJ47/LGfKGR9tFt1tVCiAa8YCLw7uWN/jbB7oz0UgbLF4RCaF4wEBjyJnmbYGvcvHoex74I0RzILDDkvad7F2in2MQ2mlNXEDoDGQWGvJqUjdCTqidYrDPPCNEJyCQw5DUkb4a+REaphBeOIDQ9PgIHSg95G7K0w1Cb96zXfRGih8SUsVrCrTrI25GtIXqeKH05ibNDoUPFssm3Vp5DXZD3Qd6maMwYvCA028asUXiI3hTLwqytLFalbGeCvD0YtEWtwmjwLMOOcoQmEmtdYJdIvpwV8vZh0hhzx74ChZAaoknEMsspKlnOD3mHMGkNc+w7s0COAgtIEJpCrMdCqlqm60DeEVzao42+892ba7f3g1+IDhdruGhxFeda+qtB3jFsGqQ/eF7qzxw6eiRyCx0o1nTBQnbfa731mpB3CpsWGR/7Lvbggh0ekDhEe4u1tOZsqn8toSsXCZ8ftNlTV4u59+LwkJhCRxBrLjFQQRjch4+9S5Mm1/ooi6PGvBCG6BhizaQFCwiBO/jYuzbneb1TwuEh/kIHizVn61bSz2+PF2cpRm+20+/Msih0jzAKvlu3LGx0Qzi8gnWIjiLWNGmo+c9vdTGX6vPCNl2evrcvpl0sYp9wstfmjiOLR2YRbUzxLAgdR6ytFPNynUvzDvcxH8cXtSmTrdi9wJzstb1hcFNQHBD70XgcRyybFP3297a06cuhIfAIVsHX4X5fGz3hsDdxxLIT+Pz4oVD13A76Vokmwl9wENyDm71O9/ta2QmH/YgjlpXA197B9viH4jo4El/Z1l3Ayl6/J9XZ5YLDzsQRyybl2j/Unov0zz+2t3UH8Aq+vs/KsVQTDrsSRyyLlOvgl2Baz9zZskmpxcPM3oBHXVmbiZNaTsQRa+sykrogNYjk06H69DXwVbW3HcvM3sAn1Tl4CYdteScRa1Ls5qXhSzUsZPBDYWq4js+FV49rYdPiy4Nb8CV40KTLjtvJXvblYQKJWGO2plLeDqMj6WGkP9ej9J/fRln66xcvMjt7yZ7S7pS53B3sy0zPDxbLTuBBvkvVBPWFy1XneuF2nC9WZHb2Ej7m2W1fFbhz/Vjp6THESn07YUki8wu+tE9pd95LhexXP2x6dqLbCSPcdjxBvMgM7aV/yYL7/pG8S/1w6sn+T8pwuqF/ObtnLcsIFZmhvXHw2DPy9qYXnj03zSN17uTP3tpCksgcg288fPaJkB3pRWhPTfRQO4+aAvR9wF5kZvZe5ibczTCdEzDLzGwC17NZ7Uqcd6IPdD0zzWNlmzWpnj/tClOPednb3HZmg6XA8/juB4Y70J0YIYVQrK1yqN4A4QmrgMws+CpmI+YcQQL7KyzY4chdL8mrVR6lULzcLIT8IjO0966n3qlHN5iHs3x8/UN9UlcY1aXFS/vIFvV0lj+MwL0s9aLblz8PTZ7msqP6QWjyjPBve2EOJ+xqFGK5RPKQtyASkUtkjvYqdAS+1up9HD/X/6m7V9RdpGpkrf6vp/ucteKNwF0Wtaj2/6TXvp8/tyuemjxTAhqdzSBqhUxdK1QsofcNJW1tlsFX8fqqhDMz7uoxsj4gbo+Kzf+vT9/NtNpzK/DLYJH64/L84+Prm5l/awReOK4Oa26uDucf3AUhVOCW+K3P1l49bcPcwqIMrf/V4bi9g8Uc9NafzIz4ayvwqbeomTJvFl2rqhV48bg6sKF5aSJb3BbhArfEEpmrva/NrKuHwPVfRuAv5mzVtRmJnS5TgdtFJu9VPx6xevrrsClwsMIcHBYeckcUInAL6c7hHHxfxwLPR+B7e0/aNALfW9nrj3qlm43A4Qpnc7gscVsKE7glfGfxtdf4a/4aHQOf+sfAjYbN80t7AvcWqbzn5x9a8uv2ENoQrkBSj8oUt6VQgVt8dx5je0347QncOwvdqqpuUdPKqnNX6o/HWejO5rMKvHrRqQm+1YudwOFB2BQSWamyxW0pXOAWp53JOfi2D4ptDDaytdeBjXn968B6+Ny/DvzIUi+6HX47mLtO9Ys8ziaI20wOodEijl37ELdlJwK3bO9c1vYS3zA4eeaSA0SCEIq2L3FbdiZwy9LOZm0v+f2+IQKTKRx8QLxPcVt2KnDLYOfzDr70t+sHCkxxNqsrye++4x2L27JzgVve32t7WfcFen0JoGwxaxUhbh8IrGhjL9u+wVJfBWlbrbc8252TFQg8d+DLrK+w1VdB20ozrc5sZzBj9wKvHviy6Dus9VVQN0/T4Cwanz37Ftj2tFXGvsRe33uMIAxxbdmzwO4nnZP3LQn6KiiaZNS4UNiK3QocdM0okchS9FUEPbFjoTHh8Db7FJjqim9UkSXpq3BtBqvGw1h6gz0KTD9fI4LI0vRV2H1/58aCwyvsTuCo063IRJao730rCAc0DhxeYl8Cp5osGSiyUH0VC4eyBD9rcHiWPQmcfqqzV9cVrK/i8W3JDyxwQDxlNwJnvVPBoSvH1tf6UfD+RD21B4eH7ENgLvcZbfZs0dH3IW5cx+Bwjz0IzMTeB0shSqi+M18nweNy4lYghuIF5hJ85xj2fGd9P46/H80Tca5V95JC/5enBG7+ZGFw+Zu1R65BBGULzNneHsqC11fnDvlx7B5ZZ55qZxL9X57isLkWx7jxBcNJrbIFlmGvxkRfx7M/+gmyt8Ope65sl+j78pR1PGZg+FTjxt4dLlZgIcFXMx48W5rSPuS5e7J7l+j/8pQ5As4qJ5Fr1w6XKbAke1eOfTfMGQrcWBn88hTr6q1I5NZ+HS5RYEn2Wp26mpqk/96KwHfvl6dQXsdNpdZOD4iLE1hU8HU88/ze4/4QeHQM7P3ylFgTMNKJtUOHyxJYlr2+131b0TpXh2ehnV+eEkvcXg3xip5UtTOHSxJYlr1B0zaaCPxr3V3rfwbXge1fnhJf3OH2pmJXDhcjsLDgGzrratBFl02cfXR7QnH7laatbS8OlyGwNHsjTZqcmjkQOIu4/eoTV7cLiUsQWJq90ec8P0zVAmcWt79dySvM/6UjI15gccE34S0LbMztSL8prL5+BGQLLM/e9HccDS895SbHRvD45pGQLLA8ezPdMNh2YBYiZ6k++7eOhliBBQbfjPf7Dvvvvs5mPaot0WGZAku0N/ft+jP9N5vIuVQq0GGJAku0N7e+mqXum0HkbCKV5rA4gUUGXxb6Kta7b0qRM3pU1AGxLIFl2stGX41F700jclaLinFYksAy7eWlr8a288YWOa9EZTgsRmChwZehvgq3zhtP5MwKFeCwDIGl2stUX41H540gcnaDpDssQWCp9nLWV+Pbd0mnduUXSPRJLfYCiw2+7PVVBHZdGpE56CPWYd4Cy7VXhL4aiq4bKjILe2Q6zFhgwfbK0VdD1nP9RebhjkCH2Qos2F5h+iqIe66HyFzUkXZAzFNgycFXoL6aGB3XSWQ+4khymKHAou2Vqq8mWr+1FJmRN2IcZiewaHtF66uI3G83ReakjQyHeQksO/iK11eTot+uiMxKGgEOMxJYuL1l6KtJ1m3nRGbmDPeTWmwEFm5vQfoqEvfa4dQudsZwdpiHwNKDb2H6arL0WtJJmpQw3CQDA4HF21uivpp8vZajx8w2pyG7wOLtLVZfTb5O+26qZyUyo01pySuw/OBbtr6KbH12+D5kNiLz2IqOjAIXYG/5+mpy9VlGz9Ecb1vXY+0AAAuOSURBVEXeDeiRTeAC7N2Jvho2CrfpmUVm43AegUsIvnvSV5Gpy65XmlNkHg5nELgIe/emrybTlSWrTFlEzj+aTy9wEfbuUl8NW4XbrMlFzuxwWoHLCL771VeRo8M6V5lW5JwOJxS4EHv3ra8mQ4f1rDHZzK5sDicTuBB7oa8hg8Jha8cXOc8BcRqBSwm+0PdB8u5KUl9kkdM7nEDgYuyFviNSd1fC6uKJnLhRogtcjL3Qdw65CrcFRhA5pcNOAldDtlcoJ/hC3yUSR5xItVGL7FaWs1i9VZ0qmEtcXKkge6HvKokvvEYtnFBki2K8xBpm3s6xUdjC8oLshb7bpFQ4RV1EIq+W4CfWONfGYrtfgnG2koIv9LUjZRhOV1W4yPMre4o1k2NjdYtKxnmLshf6OpDQ4fTXsAJEnqznKNZqnF5b0bqSdo17WUNn6OtMQoVTVTSs1dPk/irOZq3lX1zkrK9ZCfrunYQzkNPUs1S7s8gmt6dYi0tcV9iqKPtDtsiAvr6kcjj7vXx3V5HffQVZXG8+OUDDUgyGvkHsSGGDncj0Ys2mBklYhMHQN5hEYZiPwoZVkSOINZcYqKB8g6EvDWkcZmZww5zIMcSaSQsWULjB0JeQFA5zC8IDeiJHEWs76VJVn94eHz+Op1H+8/OP7ULFAH2p2bvChtrhKGJNk8bVPH2/X/sVjblW03rEGgx9Y5DkiRixawhmZAWRWFspH8eX+/3nt8+L1Xwcp/VIDcHQNxrxHWavcByxbFJ0PW1d40h/ef73QgSGvnGJr3Dk8gOJI5aVwJdepFf13A76TgmVevvyNh2qSxQY+sYndhjmHYTjiGWRcq2ql8en4Q/Fz28vM8fa8gSGvomI7DBnheOIZTmEfhQ1rOdSL5irJ/YTAGmBvinZocJaBxKxJkVYCdw/Wzaop47zc2e7mzLeR8wUzAHom5q4fYFBP5vt+XRibaTMVHQ7dIUPhuqX5ukfoytYCyNolkJD3yzE3P05Hjtv07MJxLKKt4MkU8P16Xu/niGTHwrby8AMhIa++RCssF/PHXhBJdbm9EpVijqitq/HexpHcqGhb14i7mPakol6ZqBY27K2acOKTCRfulw1qYduGlZkn6EvA+I5HPgcqyhdb+CGq1gLdw5uVxSwjaTQtir05UI8hR02IdHYL9H9wAEVJZwGHdLq0JcTsZRZLjbb2RdysZbK83v0R9Yn6tgLDX3ZEcmittTkp1cWoRaL9qmUrGZgLe006MsTSq34CDuFVqyNR8MT1cICsytfXznuVKDw3ylrwvLb05RirZdk/YoW11cy5aIffRn/SO8Xyx3hFmH57Vs6sTaL2X5Di/ML1bKxOnjmPOoiZHq1ghtzTR+4czjuTQttbMSyMm/xbWnub0PMiOOx706E5si46Qkan+f+IxDL3r5qDuu1sxN86gpCR2TSuPQNzHePBYklSMEQIpx5zi/0x/H3o5nvfq26W0s/vv5RPX1XT0xTKbcvfx6aRc3EHzWEbvJkxKbxyNuUr8IB7ELgJBeO0gv9cfz0ph/qcK3N/Di2M/LUBDyVeju8qNtbTnrx/fzZpGqBj9Mb1SLj1zjkCtMWx4EdCJzpum98ofVz0W6Hk5kS39xaqhP1P+pWFyWxvjv84+ubuQPGCPyyWjABVN+euOHKC8LFC8xm2ga9z/qEcv2PuTOtubVUJxqZ65TmpjXj9rWqWoHJz0RH/LmiLo2wMAYULjAbfScQ9PihwI2VRuD2fnD9ZActcH1Q/PTXgUzgtAcMUHiRogXmq+8URyPU8q0IfG8ezaI+6qU3f4HTH+FP6icsjK6o7BQssCR9p6wb894XeHQMfOrN1jDHwOfnH/rRD1f7IXRuYWcg3AoW34eGYgWWre+UsVF1H+xcHZ6FVnLqBw6fVeBtTlKb4Fu9LAnMUNgZoPCEQgUuTd8RjWXHX5t//rN3HdjIqa4D10H3dvjtYC4V61dp1Uo3AssQdgrdpsr5zqsUKXDh+iomvW/eyO6ph1KFnYFs60W3QkuBAu9AXwuMp/91+FW8sDOQKUxTTE6KExj69lERuESDqcKw/HYpTGDou0R5FtN8H+mNUpTA0HeD0iyGwiUJDH3tKGpITfJNJLdGMQJDXzfKsZjie8htiUIEhr5elGJx+JcQ2wxFCAx9QyhiSB3+DYQ2QQECQ18K5FscvP0iv794gaEvIdItDlaYZjNSIlxg6EuP6CF14JbL+96iBYa+8ZBrcdh2S/vSggWGvtGRGoyDNlrWFxYrMPRNhkiL96KwUIGhb2rkWRyyvXK+qUiBoW8mpA2p/TdWzLcUKDD0zYwoi0tXWJzA0JcHciz23k4R30+YwNCXFVKG1L4bKeC7iRIY+rJEhMW+ChNvBjmCBIa+nOFvsd/2Mf9ScgSGvgLgPqT22jjOX0iMwNBXEKwtLk1hEQJDX3nwDcY+m8XzmygECAx95cLUYo+NYvgtNOwFhr7iYWmx8xbx+woa5gJD31LgN6R23hxem9/AWmDoWxrMLHbdGE7b3sBYYOhbKKwsdlU40mZ4w1Zg6Fs2fIbUbpvBY5sfMBUY+u4DJhY7bQSHDX7AUmDouytYWCxVYYYCQ989kn9I7VI9H4XZCQx990xmix0q52IwM4GhL8gbjK1rZhKEWQkMfUFHPotFKcxIYOgLxmSy2LZSBgqzERj6ggWyDKkta8yuMBOBoS/YIL3FlgrH3ox1WAgMfYEdiS22qixvEGYgMPQFTiQdUtvUlFPh7AJDX+BFOotZK5xZYOgLQkhksUUluQzOKjD0BQQkGVJv1pApCGcUGPoCQuJbzFLhbAJDX0BPZIu3Cs+gcCaBoS+IRtQh9UbJyRXOIjD0BdGJZ/GGwlHqXCSDwNAXpCJSMF4tM20QTi4w9AWpiWHxWokpFU4sMPQFmaC3mIXCSQWGviAvxEPqlbJSKZxQYOgLeEBp8XJJaQxOJjD0Baygs3ipmCRBOJHA0BdwhGhIvVRGAoWTCAx9AWcoLF4oIbrCCQSGvkAA4RYvKBxU5ibRBYa+QA6BQ+rZdeMG4cgCQ18gjxCL59aMqXBUgaEvEIt/MJ5ZLZ7CEQWGvkA8nhanUziawNAXlIKPxdM14hgcSWDoCwrDfUg9zh4lCEcRGPqCQnG0OL7C9gJXM8xmhL6gbFwsHuWcWW9OLActrTItlTmzAPqCXWA/pB5m639wEWuBzUzbBfXrgr5gV1haPFRY/7utqJXE20Vsb12XEfqCPWJjcT/H+7ujWGsZ1le2qsQnNwBlsT2k7hZTirW8zF1IKAz2zobFahmtWEtL/GSEwgCsBmNqsRbSvUWEwQBoZi0mF2s+OUBDGAxAx8jiEDvm17VPtK8oaG0ASqMbUodFt/krxpZpThUFrg9AgaiLR2ElWMo6reb8/KP7++N46i35OKoLVZ+3CgAARBFrmjTNda169Qy5fXmb29KF3ADsGAKxZnTdTql/DBbruT59t6sHgJ1DIJaNrtOUy/O/1/X8/Pa5qbUf6S+fx7kXSgVg58QRazulDubjofrtoOdZf3q7n/9X/f8Xi1IB2DlxxNpM+fntZflY++OolpwnFUFgAEbEEWsz5VKXtHyyTDMdr0NgAEbEEWsrRZ8N26jndhgnQWAARsQRa+sy0qV5xEdX0nCo/tiW5SIAACRiWQk8k7T4Q2F+IiaRHv4CMCGKWIEzsc7qbPf4WBsBGIApUcSaf6LWTD3zl6vu5/4oYLkaAEAMsebvRgpREP4CME8EsXA/MADJoBeL+IkceKYOACt4C+L2RI6VFbxqAQAYiMWifSql4woA7JAkT6XcXDEkLwC7hlCsjYIsnyDv9DomAHYPmVjbpWy9ocXtZWoAAA2NWHbqzb700PlNiACAAeFiOelXDXFZFQCwRIBYsBAAwUBgAAQDgQEQDAQGQDAQGADBQGAABAOBARAMBAZAMBAYAMFAYAAE8/8BVvSF0AIky5QAAAAASUVORK5CYII=" title alt style="max-width:100%;" style="display: block; margin: auto;" /></p>
<p>To build the semi-MDP in R, we use the <code>binaryMDPWriter</code> where the model can be built using either using matrices or an hierachical structure. We first illustrate how to use the hierachical structure. First, we load the parameters:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">&gt;<span class="st"> </span>N &lt;-<span class="st"> </span><span class="dv">5</span>
&gt;<span class="st"> </span>Cf &lt;-<span class="st"> </span>-<span class="dv">10</span>
&gt;<span class="st"> </span>Cp &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">0</span>, -<span class="dv">7</span>, -<span class="dv">7</span>, -<span class="dv">5</span>)  <span class="co"># use negative numbers since the MDP optimize based on rewards</span>
&gt;<span class="st"> </span>Q &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="fl">0.9</span>, <span class="fl">0.1</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="fl">0.8</span>, <span class="fl">0.1</span>, <span class="fl">0.05</span>, <span class="fl">0.05</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="fl">0.7</span>, <span class="fl">0.1</span>, <span class="fl">0.2</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="fl">0.5</span>, 
+<span class="st">     </span><span class="fl">0.5</span>), <span class="dt">nrow =</span> <span class="dv">4</span>, <span class="dt">byrow =</span> T)</code></pre></div>
<p>and make a data frame for the states:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">&gt;<span class="st"> </span>states &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">id =</span> <span class="dv">1</span>:N -<span class="st"> </span><span class="dv">1</span>, <span class="dt">label =</span> <span class="kw">paste0</span>(<span class="st">&quot;i=&quot;</span>, <span class="dv">1</span>:N), <span class="dt">stringsAsFactors =</span> F)
&gt;<span class="st"> </span>states</code></pre></div>
<pre><code>  id label
1  0   i=1
2  1   i=2
3  2   i=3
4  3   i=4
5  4   i=5</code></pre>
<p>To build the model we need transition probabilities and the state ids for the corresponding transitions. We here do this using a function:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">&gt;<span class="st"> </span><span class="co"># transform state to id</span>
<span class="er">&gt;</span><span class="st"> </span>state2Id &lt;-<span class="st"> </span>function(i) <span class="kw">return</span>(i -<span class="st"> </span><span class="dv">1</span>)
&gt;<span class="st"> </span>
<span class="er">&gt;</span><span class="st"> </span><span class="co"># input state i and action a</span>
<span class="er">&gt;</span><span class="st"> </span>transPr &lt;-<span class="st"> </span>function(a, i) {
+<span class="st">     </span>if (a ==<span class="st"> </span><span class="dv">0</span>) {
+<span class="st">         </span>pr &lt;-<span class="st"> </span>Q[i, ]
+<span class="st">         </span>iN &lt;-<span class="st"> </span><span class="kw">which</span>(pr &gt;<span class="st"> </span><span class="dv">0</span>)
+<span class="st">         </span>pr &lt;-<span class="st"> </span>pr[iN]  <span class="co"># only consider trans pr &gt; 0</span>
+<span class="st">     </span>}
+<span class="st">     </span>if (a &gt;<span class="st"> </span><span class="dv">0</span>) {
+<span class="st">         </span>pr &lt;-<span class="st"> </span><span class="dv">1</span>
+<span class="st">         </span>iN &lt;-<span class="st"> </span><span class="dv">1</span>
+<span class="st">     </span>}
+<span class="st">     </span><span class="kw">return</span>(<span class="kw">list</span>(<span class="dt">pr =</span> pr, <span class="dt">id =</span> <span class="kw">state2Id</span>(iN)))
+<span class="st"> </span>}
&gt;<span class="st"> </span><span class="kw">transPr</span>(<span class="dv">0</span>, <span class="dv">1</span>)</code></pre></div>
<p>We can now build the model using the <code>binaryMDPWriter</code>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">&gt;<span class="st"> </span><span class="co"># Build the model which is stored in a set of binary files</span>
<span class="er">&gt;</span><span class="st"> </span>w&lt;-<span class="kw">binaryMDPWriter</span>(<span class="st">&quot;hct611-1_&quot;</span>)
&gt;<span class="st"> </span>w$<span class="kw">setWeights</span>(<span class="kw">c</span>(<span class="st">&quot;Duration&quot;</span>,<span class="st">&quot;Net reward&quot;</span>))
&gt;<span class="st"> </span>w$<span class="kw">process</span>()
&gt;<span class="st">    </span>w$<span class="kw">stage</span>()
&gt;<span class="st">       </span>w$<span class="kw">state</span>(<span class="dt">label=</span><span class="st">&quot;i=1&quot;</span>)
&gt;<span class="st">          </span>dat&lt;-<span class="kw">transPr</span>(<span class="dv">0</span>,<span class="dv">1</span>)
&gt;<span class="st">          </span>w$<span class="kw">action</span>(<span class="dt">label=</span><span class="st">&quot;no repair&quot;</span>, <span class="dt">weights=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">0</span>), <span class="dt">pr=</span>dat$pr, <span class="dt">id=</span>dat$id, <span class="dt">end=</span>T)
&gt;<span class="st">       </span>w$<span class="kw">endState</span>()
&gt;<span class="st">       </span>for (ii in <span class="dv">2</span>:(N<span class="dv">-1</span>) ) {
+<span class="st">          </span>w$<span class="kw">state</span>(<span class="dt">label=</span>states$label[ii])
+<span class="st">             </span>dat&lt;-<span class="kw">transPr</span>(<span class="dv">0</span>,ii)
+<span class="st">             </span>w$<span class="kw">action</span>(<span class="dt">label=</span><span class="st">&quot;no repair&quot;</span>, <span class="dt">weights=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">0</span>), <span class="dt">pr=</span>dat$pr, <span class="dt">id=</span>dat$id, <span class="dt">end=</span>T)
+<span class="st">             </span>dat&lt;-<span class="kw">transPr</span>(<span class="dv">1</span>,ii)
+<span class="st">             </span>w$<span class="kw">action</span>(<span class="dt">label=</span><span class="st">&quot;preventive repair&quot;</span>, <span class="dt">weights=</span><span class="kw">c</span>(<span class="dv">1</span>,Cp[ii]), <span class="dt">pr=</span>dat$pr, <span class="dt">id=</span>dat$id, <span class="dt">end=</span>T)
+<span class="st">          </span>w$<span class="kw">endState</span>()
+<span class="st">       </span>}
&gt;<span class="st">       </span>w$<span class="kw">state</span>(<span class="dt">label=</span><span class="kw">paste0</span>(<span class="st">&quot;i=&quot;</span>,N))
&gt;<span class="st">          </span>dat&lt;-<span class="kw">transPr</span>(<span class="dv">2</span>,N)
&gt;<span class="st">          </span>w$<span class="kw">action</span>(<span class="dt">label=</span><span class="st">&quot;forced repair&quot;</span>, <span class="dt">weights=</span><span class="kw">c</span>(<span class="dv">2</span>,Cf), <span class="dt">pr=</span>dat$pr, <span class="dt">id=</span>dat$id, <span class="dt">end=</span>T)
&gt;<span class="st">       </span>w$<span class="kw">endState</span>()
&gt;<span class="st">    </span>w$<span class="kw">endStage</span>()
&gt;<span class="st"> </span>w$<span class="kw">endProcess</span>()
&gt;<span class="st"> </span>w$<span class="kw">closeWriter</span>()</code></pre></div>
<p>Note that we build the model with two weights applied to each action “Duration” and “Net reward”. That is, when we specify and action we must add two weights. The process is built using first a <code>process</code> which contains a <code>stage</code> (we only specify one stage, since we have a homogeneous semi-MDP over an infinite horizon) which contains <code>state</code>s which contains <code>action</code>s. Transitions of an <code>action</code> are specified using the <code>pr</code> and <code>id</code> parameter. The model is saved in a set of files with prefix “<code>hct611-1_</code>”.</p>
<p>The model can be loaded using</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">&gt;<span class="st"> </span>mdp &lt;-<span class="st"> </span><span class="kw">loadMDP</span>(<span class="st">&quot;hct611-1_&quot;</span>)</code></pre></div>
<pre><code>Read binary files (0.00145769 sec.)
Build the HMDP (0.000120678 sec.)</code></pre>
<pre><code>Checking MDP and found no errors (2.123e-006 sec.)</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">&gt;<span class="st"> </span>mdp  <span class="co"># overall info</span></code></pre></div>
<pre><code>$binNames
[1] &quot;hct611-1_stateIdx.bin&quot;          &quot;hct611-1_stateIdxLbl.bin&quot;      
[3] &quot;hct611-1_actionIdx.bin&quot;         &quot;hct611-1_actionIdxLbl.bin&quot;     
[5] &quot;hct611-1_actionWeight.bin&quot;      &quot;hct611-1_actionWeightLbl.bin&quot;  
[7] &quot;hct611-1_transProb.bin&quot;         &quot;hct611-1_externalProcesses.bin&quot;

$timeHorizon
[1] Inf

$states
[1] 5

$founderStatesLast
[1] 5

$actions
[1] 8

$levels
[1] 1

$weightNames
[1] &quot;Duration&quot;   &quot;Net reward&quot;

$ptr
C++ object &lt;0000000000235260&gt; of class 'HMDP' &lt;0000000000246EA0&gt;

attr(,&quot;class&quot;)
[1] &quot;MDP:C++&quot;</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">&gt;<span class="st"> </span>info &lt;-<span class="st"> </span><span class="kw">infoMDP</span>(mdp)  <span class="co"># more detailed info</span>
&gt;<span class="st"> </span>info$actionDF</code></pre></div>
<pre><code>  sId aIdx             label weights   trans                pr
1   5    0         no repair     1,0     0,1           0.9,0.1
2   6    0         no repair     1,0 1,2,3,4 0.8,0.1,0.05,0.05
3   6    1 preventive repair    1,-7       0                 1
4   7    0         no repair     1,0   2,3,4       0.7,0.1,0.2
5   7    1 preventive repair    1,-7       0                 1
6   8    0         no repair     1,0     3,4           0.5,0.5
7   8    1 preventive repair    1,-5       0                 1
8   9    0     forced repair   2,-10       0                 1</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">&gt;<span class="st"> </span>info$stateDF</code></pre></div>
<pre><code>   sId stateStr label
1    0      1,0      
2    1      1,1      
3    2      1,2      
4    3      1,3      
5    4      1,4      
6    5      0,0   i=1
7    6      0,1   i=2
8    7      0,2   i=3
9    8      0,3   i=4
10   9      0,4   i=5</code></pre>
<p>Note the loaded model do not nessesary gives the same id to a state as when you built it, since the order of the nodes in the hypergraph datastructure is optimized! Given the model in memory. We now can find the optimal policy under various policies</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">&gt;<span class="st"> </span><span class="co"># Optimal policy under average reward per time unit criterion</span>
<span class="er">&gt;</span><span class="st"> </span><span class="kw">policyIteAve</span>(mdp, <span class="st">&quot;Net reward&quot;</span>, <span class="st">&quot;Duration&quot;</span>)</code></pre></div>
<pre><code>Run policy iteration under average reward criterion using 
reward 'Net reward' over 'Duration'. Iterations (g): 
1 (-0.512821) 2 (-0.446154) 3 (-0.43379) 4 (-0.43379) finished. Cpu time: 2.123e-006 sec.</code></pre>
<pre><code>[1] -0.43379</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">&gt;<span class="st"> </span><span class="kw">getPolicy</span>(mdp)</code></pre></div>
<pre><code>  sId stateLabel aIdx       actionLabel   weight
1   5        i=1    0         no repair 9.132420
2   6        i=2    0         no repair 4.794521
3   7        i=3    0         no repair 2.968037
4   8        i=4    1 preventive repair 4.566210
5   9        i=5    0     forced repair 0.000000</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">&gt;<span class="st"> </span><span class="co"># Optimal policy under expected discounted reward criterion (use both policy and value ite)</span>
<span class="er">&gt;</span><span class="st"> </span><span class="kw">policyIteDiscount</span>(mdp, <span class="st">&quot;Net reward&quot;</span>, <span class="st">&quot;Duration&quot;</span>, <span class="dt">discountFactor =</span> <span class="fl">0.9</span>)</code></pre></div>
<pre><code>Run policy iteration using quantity 'Net reward' under discounting criterion 
with 'Duration' as duration using discount factor 0.9. 
Iteration(s): 1 2 3 finished. Cpu time: 2.123e-006 sec.</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">&gt;<span class="st"> </span><span class="kw">getPolicy</span>(mdp)</code></pre></div>
<pre><code>  sId stateLabel aIdx       actionLabel     weight
1   5        i=1    0         no repair  -2.662995
2   6        i=2    0         no repair  -5.621877
3   7        i=3    0         no repair  -7.713425
4   8        i=4    1 preventive repair  -7.396695
5   9        i=5    0     forced repair -12.157026</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">&gt;<span class="st"> </span><span class="kw">valueIte</span>(mdp, <span class="st">&quot;Net reward&quot;</span>, <span class="st">&quot;Duration&quot;</span>, <span class="dt">discountFactor =</span> <span class="fl">0.9</span>, <span class="dt">eps =</span> <span class="fl">1e-10</span>, <span class="dt">maxIte =</span> <span class="dv">1000</span>)</code></pre></div>
<pre><code>Run value iteration with epsilon = 1e-010 at most 1000 time(s)
using quantity 'Net reward' under expected discounted reward criterion 
with 'Duration' as duration using discount factor 0.9.
Iterations: 207 Finished. Cpu time 0.000365927 sec.</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">&gt;<span class="st"> </span><span class="kw">getPolicy</span>(mdp)</code></pre></div>
<pre><code>  sId stateLabel aIdx       actionLabel     weight
1   5        i=1    0         no repair  -2.662995
2   6        i=2    0         no repair  -5.621877
3   7        i=3    0         no repair  -7.713425
4   8        i=4    1 preventive repair  -7.396695
5   9        i=5    0     forced repair -12.157026</code></pre>
<p>The model can also be built using by specifying a set of matrices. Note this way of specifying <strong>only work</strong> for infinite-horizon semi-MDPs (and not finite-horizon or hierachical models). Specify a list of probability matrices (one for each action) where each row/state contains the transition probablities, a matrix with rewards and a matrix with stage lengths (row = state, column = action).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">&gt;<span class="st"> </span>## define probability matrices
<span class="er">&gt;</span><span class="st"> </span>P &lt;-<span class="st"> </span><span class="kw">list</span>()
&gt;<span class="st"> </span><span class="co"># a=1 (no repair)</span>
<span class="er">&gt;</span><span class="st"> </span>P[[<span class="dv">1</span>]] &lt;-<span class="st"> </span><span class="kw">as.matrix</span>(<span class="kw">rbind</span>(Q, <span class="dv">0</span>))
&gt;<span class="st"> </span><span class="co"># a=2 (preventive repair)</span>
<span class="er">&gt;</span><span class="st"> </span>Z &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">0</span>, <span class="dt">nrow =</span> N, <span class="dt">ncol =</span> N)
&gt;<span class="st"> </span>Z[<span class="dv">2</span>, <span class="dv">1</span>] &lt;-<span class="st"> </span>Z[<span class="dv">3</span>, <span class="dv">1</span>] &lt;-<span class="st"> </span>Z[<span class="dv">4</span>, <span class="dv">1</span>] &lt;-<span class="st"> </span><span class="dv">1</span>
&gt;<span class="st"> </span>P[[<span class="dv">2</span>]] &lt;-<span class="st"> </span>Z
&gt;<span class="st"> </span><span class="co"># a=3 (forced repair)</span>
<span class="er">&gt;</span><span class="st"> </span>Z &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">0</span>, <span class="dt">nrow =</span> N, <span class="dt">ncol =</span> N)
&gt;<span class="st"> </span>Z[<span class="dv">5</span>, <span class="dv">1</span>] &lt;-<span class="st"> </span><span class="dv">1</span>
&gt;<span class="st"> </span>P[[<span class="dv">3</span>]] &lt;-<span class="st"> </span>Z
&gt;<span class="st"> </span><span class="co"># reward 6x3 matrix with one column for each action</span>
<span class="er">&gt;</span><span class="st"> </span>R &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">0</span>, <span class="dt">nrow =</span> N, <span class="dt">ncol =</span> <span class="dv">3</span>)
&gt;<span class="st"> </span>R[<span class="dv">2</span>:<span class="dv">4</span>, <span class="dv">2</span>] &lt;-<span class="st"> </span>Cp[<span class="dv">2</span>:<span class="dv">4</span>]
&gt;<span class="st"> </span>R[<span class="dv">5</span>, <span class="dv">3</span>] &lt;-<span class="st"> </span>Cf
&gt;<span class="st"> </span><span class="co"># state lengths</span>
<span class="er">&gt;</span><span class="st"> </span>D &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">1</span>, <span class="dt">nrow =</span> N, <span class="dt">ncol =</span> <span class="dv">3</span>)
&gt;<span class="st"> </span>D[<span class="dv">5</span>, <span class="dv">3</span>] &lt;-<span class="st"> </span><span class="dv">2</span>
&gt;<span class="st"> </span>
<span class="er">&gt;</span><span class="st"> </span><span class="co"># build model</span>
<span class="er">&gt;</span><span class="st"> </span>w &lt;-<span class="st"> </span><span class="kw">binaryMDPWriter</span>(<span class="st">&quot;hct611-2_&quot;</span>)
&gt;<span class="st"> </span>w$<span class="kw">setWeights</span>(<span class="kw">c</span>(<span class="st">&quot;Duration&quot;</span>, <span class="st">&quot;Net reward&quot;</span>))
&gt;<span class="st"> </span>w$<span class="kw">process</span>(P, R, D)
&gt;<span class="st"> </span>w$<span class="kw">closeWriter</span>()</code></pre></div>
<pre><code>
  Statistics:
    states : 5 
    actions: 8 
    weights: 2 

  Closing binary MDP writer.</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">&gt;<span class="st"> </span>mdp &lt;-<span class="st"> </span><span class="kw">loadMDP</span>(<span class="st">&quot;hct611-2_&quot;</span>)</code></pre></div>
<pre><code>Read binary files (0.0218827 sec.)
Build the HMDP (0.000103338 sec.)</code></pre>
<pre><code>Checking MDP and found no errors (2.477e-006 sec.)</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">&gt;<span class="st"> </span><span class="kw">policyIteAve</span>(mdp, <span class="st">&quot;Net reward&quot;</span>, <span class="st">&quot;Duration&quot;</span>)</code></pre></div>
<pre><code>Run policy iteration under average reward criterion using 
reward 'Net reward' over 'Duration'. Iterations (g): 
1 (-0.512821) 2 (-0.446154) 3 (-0.43379) 4 (-0.43379) finished. Cpu time: 2.477e-006 sec.</code></pre>
<pre><code>[1] -0.43379</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">&gt;<span class="st"> </span><span class="kw">getPolicy</span>(mdp)</code></pre></div>
<pre><code>  sId stateLabel aIdx actionLabel   weight
1   5          1    0           1 9.132420
2   6          2    0           1 4.794521
3   7          3    0           1 2.968037
4   8          4    1           2 4.566210
5   9          5    0           3 0.000000</code></pre>
<!-- ## A finite Semi-MDP  -->
<!-- A *finite-horizon semi-MDP* considers a sequential decision problem over $N$ *stages*. Let $I_{n}$ -->
<!-- denote the finite set of system states at stage $n$. When *state* $i \in I_{n}$ is observed, an -->
<!-- *action* $a$ from the finite set of allowable actions $A_n(i)$ must be chosen, and this decision -->
<!-- generates *reward* $r_{n}(i,a)$. Moreover, let $\tau_n(i,a)$ denote the *stage length* of action -->
<!-- $a$, i.e. the expected time until the next decision epoch (stage $n+1$) given action $a$ and state -->
<!-- $i$. Finally, let $p_{ij}(a,n)$ denote the *transition probability* of obtaining state $j\in I_{n+1}$ -->
<!-- at stage $n+1$ given that action $a$ is chosen in state $i$ at stage $n$. A policy is a decision -->
<!-- rule/function that assigns to each state in a process an action. -->
<!-- A hypergraph -->
<!-- representing an MDP with time-horizon $N=5$ is shown in Figure 1. Each node corresponds to a -->
<!-- specific state in the MDP and a directed hyperarc is defined for each possible action. For instance, -->
<!-- node $v_{2,1}$ corresponds to a state number 1 at stage 2. The two hyperarcs with head in node -->
<!-- $v_{3,0}$ show that two actions are possible given state number 0 at stage 3. Action `mt` -->
<!-- corresponds to a deterministic transition to state number zero at stage 4 and action `nmt` -->
<!-- corresponds to a transition to state number 0 or 1 at stage 4 with a certain probability greater -->
<!-- than zero. -->
<!-- ![Figure 1: A state-expanded hypergraph for an MDP.](mdp_example_files/state_hgf.png) -->
<!-- *Figure 1: A state-expanded hypergraph for an MDP with time horizon $N=5$. At stage $n$ each node $v_{n,i}$ corresponds to a state in $\mathcal{S}_n$. The hyperarcs correspond to actions, e.g. if the system at stage $3$ is in state number 1 then there are two possible actions. Action `mt` results in a deterministic transition to state zero (because there is only one tail) at stage $4$ and `nmt` results in a transition to either state number 1 or 2 with a certain probability.* -->
<!-- States and actions can be identified using either an *unique id* or *state vector* -->
<!-- $\nu$. In an ordinary MDP the index vector consists of the stage and state -->
<!-- number, i.e. state corresponding to node $v_{3,1}$ in Figure 1 is -->
<!-- uniquely identified using $\nu=(n,s)=(3,1)$. Similar, action `buy` is -->
<!-- uniquely identified using $\nu=(n,s,a)=(0,0,0)$. Note that numbers in an **index always -->
<!-- start from zero**. -->
<!-- A hierarchical MDP is an MDP with parameters defined in a special way, but -->
<!-- nevertheless in accordance with all usual rules and conditions relating to such -->
<!-- processes (@Kristensen00). The basic idea of the hierarchical structure is -->
<!-- that stages of the process can be expanded to a so-called *child process*, -->
<!-- which again may expand stages further to new child processes leading to -->
<!-- multiple levels. To illustrate consider the MDP shown in Figure 2. The -->
<!-- process has three levels. At `Level 2` we have a set of ordinary MDPs -->
<!-- with finite time-horizon (one for each oval box) which all can be represented -->
<!-- using a state-expanded hypergraph (hyperarcs not shown, only hyperarcs -->
<!-- connecting processes are shown). An MDP at `Level 2` is uniquely defined -->
<!-- by a given state $s$ and action $a$ of its *parent process* at -->
<!-- `Level 1` (illustrated by the arcs with head and tail node at -->
<!-- `Level 1` and `Level 2`, respectively). Moreover, when a child -->
<!-- process at `Level 2` terminates a transition from a state $s\in -->
<!-- \mathcal{S}_{N}$ of the child process to a state at the next stage of the -->
<!-- parent process occur (illustrated by the (hyper)arcs having head and tail at -->
<!-- `Level 2` and `Level 1`, respectively). Since a child process is -->
<!-- always defined by a stage, state and action of the parent process we have that -->
<!-- for instance a state at level 1 have an index vector -->
<!-- $\nu=(n_{0},s_{0},a_{0},n_{1},s_{1})$. -->
<!-- ![Figure 2: A state-expanded hypergraph for an HMDP.](mdp_example_files/hmdp_index.png) -->
<!-- *Figure 2: A hypergraph representation of the first stage of a hierarchical MDP. Level 0 indicate the -->
<!-- founder level, and the nodes indicates states at the different levels. A child -->
<!-- process (oval box) is represented using its state-expanded hypergraph (hyperarcs -->
<!-- not shown) and is uniquely defined by a given state and action of its parent -->
<!-- process.* -->
<!-- In general a state $s$ and action $a$ at level $l$ can be uniquely identified -->
<!-- using -->
<!-- $$ -->
<!-- \begin{aligned} -->
<!-- \nu_{s}&=(n_{0},s_{0},a_{0},n_{1},s_{1},\ldots,n_{l},s_{l}) \\ -->
<!-- \nu_{a}&=(n_{0},s_{0},a_{0},n_{1},s_{1},\ldots,n_{l},s_{l},a_{l}). -->
<!-- \end{aligned} -->
<!-- $$ -->
<!-- The index vector of three states is illustrated in Figure 2. -->
<!-- Another way to identify a state or action is using an id number. Id numbers can -->
<!-- be seen when printing information about the model i R. This will be further -->
<!-- clarified in the example in the next section. -->
<!-- ## Figures -->
<!-- The figure sizes have been customised so that you can easily put two images side-by-side.  -->
<!-- ```{r, fig.show='hold'} -->
<!-- plot(1:10) -->
<!-- plot(10:1) -->
<!-- ``` -->
<!-- You can enable figure captions by `fig_caption: yes` in YAML: -->
<!--     output: -->
<!--       rmarkdown::html_vignette: -->
<!--         fig_caption: yes -->
<!-- Then you can use the chunk option `fig.cap = "Your figure caption."` in **knitr**. -->
<!-- ## More Examples -->
<!-- You can write math expressions, e.g. $Y = X\beta + \epsilon$, footnotes^[A footnote here.], and tables, e.g. using `knitr::kable()`. -->
<!-- ```{r, echo=FALSE, results='asis'} -->
<!-- knitr::kable(head(mtcars, 10)) -->
<!-- ``` -->
<!-- Also a quote using `>`: -->
<!-- > "He who gives up [code] safety for [code] speed deserves neither." -->
<!-- ([via](https://twitter.com/hadleywickham/status/504368538874703872)) -->
</div>
<div id="references" class="section level2 unnumbered">
<h2>References</h2>
<div id="refs" class="references">
<div id="ref-Kristensen00">
<p>Kristensen, A. R., and E. Jørgensen. 2000. “Multi-Level Hierarchic Markov Processes as a Framework for Herd Management Support.” <em>Annals of Operations Research</em> 94: 69–89. doi:<a href="https://doi.org/10.1023/A:1018921201113">10.1023/A:1018921201113</a>.</p>
</div>
<div id="ref-Relund06">
<p>Nielsen, L.R., and A.R. Kristensen. 2006. “Finding the <span class="math inline">\(K\)</span> Best Policies in a Finite-Horizon Markov Decision Process.” <em>European Journal of Operational Research</em> 175 (2). Danish Informatics Network in the Agriculture Sciences (DINA), The Royal Veterinary; Agricultural University: 1164–79. doi:<a href="https://doi.org/10.1016/j.ejor.2005.06.011">10.1016/j.ejor.2005.06.011</a>.</p>
</div>
<div id="ref-Puterman94">
<p>Puterman, M.L. 1994. <em>Markov Decision Processes</em>. Wiley Series in Probability and Mathematical Statistics. Wiley-Interscience.</p>
</div>
<div id="ref-Tijms03">
<p>Tijms, Henk. C. 2003. <em>A First Course in Stochastic Models</em>. John Wiley &amp; Sons Ltd.</p>
</div>
</div>
</div>



<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
